import math
import os
import re
import time
import json
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, asdict

import gradio as gr
import librosa
import numpy as np
import soundfile as sf
import matplotlib.pyplot as plt
from scipy import signal

# ========= è§£å†³ä¸­æ–‡å­—ä½“é—®é¢˜ =========
import matplotlib
try:
    # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
    font_list = []
    
    # Windowså­—ä½“è·¯å¾„
    if os.name == 'nt':
        font_dirs = [
            'C:/Windows/Fonts',  # Windowsç³»ç»Ÿå­—ä½“
            os.path.expanduser('~/.fonts'),  # ç”¨æˆ·å­—ä½“
        ]
        # å¸¸è§ä¸­æ–‡å­—ä½“åç§°
        chinese_fonts = [
            'msyh.ttc',  # å¾®è½¯é›…é»‘
            'simhei.ttf',  # é»‘ä½“
            'simsun.ttc',  # å®‹ä½“
            'simkai.ttf',  # æ¥·ä½“
            'STHeiti Light.ttc',  # åæ–‡é»‘ä½“ï¼ˆMacï¼‰
            'PingFang.ttc',  # è‹¹æ–¹ï¼ˆMacï¼‰
        ]
    else:
        # Linux/Macå­—ä½“è·¯å¾„
        font_dirs = [
            '/usr/share/fonts',
            '/usr/local/share/fonts',
            os.path.expanduser('~/.fonts'),
            os.path.expanduser('~/Library/Fonts'),  # Mac
        ]
        chinese_fonts = [
            'wqy-microhei.ttc',  # æ–‡æ³‰é©¿å¾®ç±³é»‘
            'NotoSansCJK-Regular.ttc',  # Notoå­—ä½“
            'SourceHanSansSC-Regular.otf',  # æ€æºé»‘ä½“
        ]
    
    # æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    available_fonts = []
    for font_dir in font_dirs:
        if os.path.exists(font_dir):
            for font_file in chinese_fonts:
                font_path = os.path.join(font_dir, font_file)
                if os.path.exists(font_path):
                    available_fonts.append(font_path)
                    print(f"æ‰¾åˆ°ä¸­æ–‡å­—ä½“: {font_path}")
    
    # å¦‚æœæœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
    if available_fonts:
        matplotlib.font_manager.fontManager.addfont(available_fonts[0])
        font_name = matplotlib.font_manager.FontProperties(fname=available_fonts[0]).get_name()
        matplotlib.rcParams['font.sans-serif'] = [font_name]
        matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
        print(f"å·²è®¾ç½®ä¸­æ–‡å­—ä½“: {font_name}")
    else:
        print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“")
        
except Exception as e:
    print(f"è®¾ç½®å­—ä½“æ—¶å‡ºé”™: {e}")
    # ä½¿ç”¨é»˜è®¤å­—ä½“ï¼Œä½†å°è¯•è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
    matplotlib.rcParams['axes.unicode_minus'] = False

# ========= é…ç½®ç®¡ç† =========

@dataclass
class AppConfig:
    # éŸ³é¢‘å¤„ç†å‚æ•°
    sample_rate: int = 22050
    min_freq: float = 32.70  # C1
    max_freq: float = 4186.01  # C8
    tempo: int = 120  # BPM
    beat_duration: float = 60.0 / 120  # ä¸€æ‹çš„æ—¶é—´ï¼ˆç§’ï¼‰
    beat_division: int = 4  # æ¯æ‹åˆ†å‰²æ•°ï¼ˆ4è¡¨ç¤º16åˆ†éŸ³ç¬¦ï¼‰
    time_stretch_range: Tuple[float, float] = (0.5, 2.0)  # æ—¶é—´æ‹‰ä¼¸èŒƒå›´
    pitch_shift_range: Tuple[int, int] = (-12, 12)  # éŸ³é«˜ç§»åŠ¨èŒƒå›´ï¼ˆåŠéŸ³ï¼‰
    
    # æ£€æµ‹å‚æ•°
    silence_threshold_db: float = 40
    min_clip_duration: float = 0.05
    confidence_threshold: float = 0.7
    
    # æ–‡ä»¶è·¯å¾„
    clip_dir: str = "clips"
    output_dir: str = "output"
    config_file: str = "config.json"

config = AppConfig()
os.makedirs(config.clip_dir, exist_ok=True)
os.makedirs(config.output_dir, exist_ok=True)

# ========= éŸ³é¢‘å¤„ç†å·¥å…·å‡½æ•° =========

_clip_index_cache = None  # ç¼“å­˜ç´¢å¼•ï¼Œé¿å…æ¯æ¬¡é‡å»º

def build_clip_index():
    """
    æ„å»ºéŸ³é¢‘ç‰‡æ®µçš„éŸ³é«˜ç´¢å¼•ï¼ŒåŠ é€ŸæŸ¥æ‰¾ã€‚
    è¿”å›ç»“æ„: {rounded_midi: [(clip_info, original_midi, confidence), ...]}
    """
    global _clip_index_cache
    if _clip_index_cache is not None:
        return _clip_index_cache
    
    index = {}
    available_clips = clip_manager.get_all_clips()
    
    for clip in available_clips:
        note_info = clip.get('note_info', {})
        if note_info and note_info.get('frequency') and note_info.get('confidence'):
            clip_freq = note_info['frequency']
            clip_midi = freq_to_midi(clip_freq)  # ç²¾ç¡®çš„æµ®ç‚¹æ•°MIDI
            confidence = note_info.get('confidence', 0.5)
            
            # å°†MIDIéŸ³é«˜å››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„æ•´æ•°ï¼Œä½œä¸ºç´¢å¼•é”®
            rounded_midi = int(round(clip_midi))
            
            if rounded_midi not in index:
                index[rounded_midi] = []
            
            index[rounded_midi].append({
                'clip': clip,
                'exact_midi': clip_midi,     # ä¿å­˜ç²¾ç¡®å€¼ç”¨äºè®¡ç®—
                'confidence': confidence,
                # å¯ä»¥åœ¨è¿™é‡Œæ‰©å±•å­˜å‚¨ instrument_tag ç­‰å…ƒæ•°æ®
            })
    
    _clip_index_cache = index
    print(f"[ç´¢å¼•æ„å»ºå®Œæˆ] å…± {len(available_clips)} ä¸ªç‰‡æ®µï¼Œç´¢å¼•åˆ° {len(index)} ä¸ªä¸åŒéŸ³é«˜ã€‚")
    return index

# ========= ä¹ç†å·¥å…·å‡½æ•° =========

def note_to_midi(note: str) -> Optional[int]:
    """éŸ³åè½¬MIDIç¼–å·ï¼Œæ”¯æŒæ‰©å±•æ ¼å¼"""
    if not note:
        return None
    
    note = note.strip().upper()
    
    # åŒ¹é…æ ¼å¼ï¼šéŸ³å[å‡é™å·]å…«åº¦ï¼ˆä¾‹å¦‚ï¼šC4, D#4, Gb3, Aâ™¯5ï¼‰
    pattern = r'([A-G])([#â™¯bâ™­]?)(-?\d+)'
    match = re.match(pattern, note)
    
    if not match:
        return None
    
    note_name, accidental, octave_str = match.groups()
    
    # åŸºæœ¬éŸ³åæ˜ å°„
    base_notes = {'C': 0, 'D': 2, 'E': 4, 'F': 5, 'G': 7, 'A': 9, 'B': 11}
    
    if note_name not in base_notes:
        return None
    
    midi_base = base_notes[note_name]
    
    # å¤„ç†å‡é™å·
    if accidental in ('#', 'â™¯'):
        midi_base += 1
    elif accidental in ('b', 'â™­'):
        midi_base -= 1
    
    try:
        octave = int(octave_str)
        midi_number = (octave + 1) * 12 + midi_base
        return midi_number
    except ValueError:
        return None

def midi_to_note(midi: int) -> str:
    """MIDIç¼–å·è½¬éŸ³å"""
    if not 0 <= midi <= 127:
        return ""
    
    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    octave = midi // 12 - 1
    note_name = notes[midi % 12]
    
    # å°†å‡å·æ›¿æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
    if '#' in note_name:
        base_note = note_name[0]
        return f"{base_note}â™¯{octave}"
    return f"{note_name}{octave}"

def midi_to_freq(midi: int) -> float:
    """MIDIè½¬é¢‘ç‡"""
    return 440.0 * (2.0 ** ((midi - 69) / 12.0))

def freq_to_midi(freq: float) -> float:
    """é¢‘ç‡è½¬MIDIï¼ˆæµ®ç‚¹æ•°ï¼Œæ›´ç²¾ç¡®ï¼‰"""
    return 69 + 12 * math.log2(freq / 440.0)

def freq_to_note(freq: float) -> Tuple[str, float]:
    """é¢‘ç‡è½¬éŸ³åå’ŒéŸ³åˆ†åå·®"""
    midi_float = freq_to_midi(freq)
    midi_int = round(midi_float)
    
    # è®¡ç®—éŸ³åˆ†åå·®
    cents = (midi_float - midi_int) * 100
    
    note_name = midi_to_note(midi_int)
    return note_name, cents

def detect_pitch_advanced(y: np.ndarray, sr: int) -> Dict[str, Any]:
    """
    é«˜çº§éŸ³é«˜æ£€æµ‹ï¼Œè¿”å›è¯¦ç»†ä¿¡æ¯
    """
    if y.ndim > 1:
        y = np.mean(y, axis=1)
    
    # è£å‰ªé™éŸ³
    y_trimmed, _ = librosa.effects.trim(
        y, 
        top_db=config.silence_threshold_db,
        frame_length=2048,
        hop_length=512
    )
    
    if len(y_trimmed) < sr * config.min_clip_duration:
        return {
            'frequency': None,
            'note': None,
            'cents': 0,
            'confidence': 0,
            'stable': False
        }
    
    # ä½¿ç”¨å¤šç§æ–¹æ³•æ£€æµ‹éŸ³é«˜
    freqs = []
    confidences = []
    
    # æ–¹æ³•1: YINç®—æ³•
    f0_yin = librosa.yin(
        y_trimmed,
        fmin=config.min_freq,
        fmax=config.max_freq,
        sr=sr,
        frame_length=2048,
        hop_length=512
    )
    f0_yin = f0_yin[f0_yin > 0]
    if len(f0_yin) > 0:
        freqs.append(np.median(f0_yin))
        # ç”¨ç¨³å®šåº¦ä½œä¸ºç½®ä¿¡åº¦
        confidences.append(1.0 - (np.std(f0_yin) / np.mean(f0_yin)) if np.mean(f0_yin) > 0 else 0)
    
    # æ–¹æ³•2: PYINç®—æ³•ï¼ˆæ›´å‡†ç¡®ä½†æ›´æ…¢ï¼‰
    try:
        f0_pyin, voiced_flag, voiced_probs = librosa.pyin(
            y_trimmed,
            fmin=config.min_freq,
            fmax=config.max_freq,
            sr=sr
        )
        f0_pyin = f0_pyin[voiced_flag]
        if len(f0_pyin) > 0:
            freqs.append(np.median(f0_pyin))
            confidences.append(np.mean(voiced_probs[voiced_flag]))
    except:
        pass
    
    # æ–¹æ³•3: è°æ³¢ä¹˜ç§¯è°±ï¼ˆå¯¹éŸ³ä¹ä¿¡å·æ›´å‡†ç¡®ï¼‰
    try:
        f0_hps = _detect_pitch_hps(y_trimmed, sr)
        if f0_hps:
            freqs.append(f0_hps)
            confidences.append(0.7)
    except:
        pass
    
    if not freqs:
        return {
            'frequency': None,
            'note': None,
            'cents': 0,
            'confidence': 0,
            'stable': False
        }
    
    # åŠ æƒå¹³å‡
    weights = np.array(confidences)
    if weights.sum() == 0:
        weights = np.ones(len(freqs))
    
    weighted_freq = np.average(freqs, weights=weights)
    avg_confidence = np.mean(confidences)
    
    # è½¬æ¢ä¸ºéŸ³å
    note_name, cents = freq_to_note(weighted_freq)
    
    # åˆ¤æ–­æ˜¯å¦ç¨³å®š
    is_stable = avg_confidence > config.confidence_threshold and len(y_trimmed) > sr * 0.2
    
    return {
        'frequency': float(weighted_freq) if weighted_freq is not None else None,
        'note': note_name,
        'cents': float(cents) if cents is not None else None,
        'confidence': float(avg_confidence),
        'stable': bool(is_stable),
        'duration': float(len(y_trimmed) / sr)
    }

def _detect_pitch_hps(y: np.ndarray, sr: int) -> Optional[float]:
    """è°æ³¢ä¹˜ç§¯è°±éŸ³é«˜æ£€æµ‹"""
    n_fft = 2048
    hop_length = 512
    
    # è®¡ç®—é¢‘è°±
    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))
    
    # è°æ³¢ä¹˜ç§¯è°±
    hps = S.copy()
    for harmonic in (2, 3, 4):
        downsampled = signal.resample_poly(
            S, 
            1, 
            harmonic, 
            axis=0
        )
        hps = hps[:downsampled.shape[0]] * downsampled
    
    # å¯»æ‰¾å³°å€¼
    hps_mean = np.mean(hps, axis=1)
    peaks, properties = signal.find_peaks(hps_mean, height=np.max(hps_mean)*0.1)
    
    if len(peaks) == 0:
        return None
    
    # æ‰¾åˆ°æœ€ä½é¢‘ç‡çš„å³°å€¼ï¼ˆåŸºé¢‘ï¼‰
    min_peak_idx = peaks[np.argmin(peaks)]
    freq = librosa.fft_frequencies(sr=sr, n_fft=n_fft)[min_peak_idx]
    
    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    if config.min_freq <= freq <= config.max_freq:
        return freq
    return None

# ========= éŸ³é¢‘å¤„ç†å‡½æ•° =========

def time_stretch(y: np.ndarray, sr: int, target_duration: float) -> np.ndarray:
    """
    æ—¶é—´æ‹‰ä¼¸ï¼ˆå˜é€Ÿä¸å˜è°ƒï¼‰
    """
    current_duration = len(y) / sr
    rate = current_duration / target_duration
    
    # é™åˆ¶æ‹‰ä¼¸èŒƒå›´
    rate = np.clip(rate, *config.time_stretch_range)
    
    # ä½¿ç”¨librosaçš„æ—¶é—´æ‹‰ä¼¸
    y_stretched = librosa.effects.time_stretch(y, rate=rate)
    
    # ç¡®ä¿é•¿åº¦å‡†ç¡®
    target_samples = int(target_duration * sr)
    if len(y_stretched) > target_samples:
        y_stretched = y_stretched[:target_samples]
    else:
        y_stretched = np.pad(
            y_stretched, 
            (0, target_samples - len(y_stretched)), 
            mode='constant'
        )
    
    return y_stretched

def pitch_shift(y: np.ndarray, sr: int, semitones: float) -> np.ndarray:
    """
    éŸ³é«˜å¹³ç§»ï¼ˆå˜è°ƒä¸å˜é€Ÿï¼‰
    """
    # é™åˆ¶ç§»åŠ¨èŒƒå›´
    semitones = np.clip(semitones, *config.pitch_shift_range)
    
    return librosa.effects.pitch_shift(
        y, 
        sr=sr, 
        n_steps=semitones,
        bins_per_octave=12
    )

def normalize_audio(y: np.ndarray) -> np.ndarray:
    """éŸ³é¢‘å½’ä¸€åŒ–"""
    if len(y) == 0:
        return y
    
    max_amp = np.max(np.abs(y))
    if max_amp > 0:
        return y / max_amp * 0.9  # ç•™ä¸€ç‚¹headroom
    return y

def apply_fade(y: np.ndarray, sr: int, fade_in: float = 0.01, fade_out: float = 0.01) -> np.ndarray:
    """åº”ç”¨æ·¡å…¥æ·¡å‡º"""
    if len(y) == 0:
        return y
    
    fade_in_samples = int(fade_in * sr)
    fade_out_samples = int(fade_out * sr)
    
    # åˆ›å»ºæ·¡å…¥æ·¡å‡ºçª—å£
    if fade_in_samples > 0:
        fade_in_window = np.linspace(0, 1, fade_in_samples)
        if fade_in_samples <= len(y):
            y[:fade_in_samples] *= fade_in_window
    
    if fade_out_samples > 0:
        fade_out_window = np.linspace(1, 0, fade_out_samples)
        if fade_out_samples <= len(y):
            y[-fade_out_samples:] *= fade_out_window
    
    return y

# ========= é¢‘è°±å›¾å¯è§†åŒ– =========

def create_spectrogram(y: np.ndarray, sr: int, detected_freq: float = None) -> plt.Figure:
    """
    åˆ›å»ºæ›´æ˜“è¯»çš„é¢‘è°±å›¾ï¼ŒåŒ…å«ä¸­æ–‡æ ‡ç­¾å’Œè¯¦ç»†è¯´æ˜
    
    é¢‘è°±å›¾è§£é‡Šï¼š
    - Xè½´ï¼šæ—¶é—´ï¼ˆç§’ï¼‰
    - Yè½´ï¼šé¢‘ç‡ï¼ˆèµ«å…¹Hzï¼‰ï¼Œå¯¹æ•°åæ ‡æ˜¾ç¤ºï¼ˆä½éŸ³åœ¨ä¸‹ï¼Œé«˜éŸ³åœ¨ä¸Šï¼‰
    - é¢œè‰²ï¼šéŸ³é‡å¼ºåº¦ï¼ˆæ·±è‰²=å®‰é™ï¼Œäº®è‰²=å“äº®ï¼‰
    - æ°´å¹³çº¿ï¼šæ£€æµ‹åˆ°çš„åŸºé¢‘
    """
    plt.figure(figsize=(12, 8))
    
    # è®¡ç®—é¢‘è°±å›¾
    n_fft = 2048
    hop_length = 512
    
    # ä½¿ç”¨melé¢‘è°±å›¾ï¼Œæ›´ç¬¦åˆäººè€³å¬è§‰
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)
    S_dB = librosa.power_to_db(S, ref=np.max)
    
    # æ˜¾ç¤ºé¢‘è°±å›¾
    ax1 = plt.subplot(2, 1, 1)
    img = librosa.display.specshow(S_dB, sr=sr, hop_length=hop_length, 
                                   x_axis='time', y_axis='mel',
                                   cmap='viridis')
    
    plt.colorbar(img, format='%+2.0f dB', ax=ax1)
    plt.title('éŸ³é¢‘é¢‘è°±å›¾ (Mel Spectrogram)', fontsize=14, fontweight='bold')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('é¢‘ç‡ (Hz) - Melåˆ»åº¦')
    
    # æ ‡è®°æ£€æµ‹åˆ°çš„é¢‘ç‡
    if detected_freq:
        # å°†é¢‘ç‡è½¬æ¢ä¸ºmelåˆ»åº¦
        mel_freq = librosa.hz_to_mel(detected_freq)
        plt.axhline(y=mel_freq, color='red', linestyle='--', linewidth=2, 
                   alpha=0.8, label=f'æ£€æµ‹åŸºé¢‘: {detected_freq:.1f} Hz')
        
        # åœ¨å³ä¾§æ˜¾ç¤ºé¢‘ç‡å€¼
        plt.text(plt.xlim()[1] * 1.02, mel_freq, 
                f'{detected_freq:.0f} Hz', 
                color='red', va='center', fontsize=10)
        
        plt.legend(loc='upper right')
    
    # æ·»åŠ ç½‘æ ¼ï¼Œæé«˜å¯è¯»æ€§
    plt.grid(True, alpha=0.3, linestyle='--')
    
    # åœ¨ä¸‹æ–¹æ˜¾ç¤ºæ³¢å½¢å›¾
    ax2 = plt.subplot(2, 1, 2)
    time = np.linspace(0, len(y)/sr, len(y))
    plt.plot(time, y, color='blue', alpha=0.7, linewidth=0.5)
    plt.fill_between(time, y, 0, alpha=0.3, color='blue')
    
    plt.title('éŸ³é¢‘æ³¢å½¢', fontsize=14, fontweight='bold')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('æŒ¯å¹…')
    plt.grid(True, alpha=0.3, linestyle='--')
    
    # è®¾ç½®xè½´èŒƒå›´ä¸€è‡´
    ax1.set_xlim([0, len(y)/sr])
    ax2.set_xlim([0, len(y)/sr])
    
    plt.tight_layout()
    
    return plt.gcf()

def create_enhanced_analysis(y: np.ndarray, sr: int, detected_info: Dict) -> plt.Figure:
    """
    åˆ›å»ºå¢å¼ºåˆ†æå›¾ï¼ŒåŒ…å«å¤šç§å¯è§†åŒ–
    """
    fig = plt.figure(figsize=(15, 10))
    
    # 1. é¢‘è°±å›¾
    ax1 = plt.subplot(3, 2, 1)
    S = librosa.feature.melspectrogram(y=y, sr=sr)
    S_dB = librosa.power_to_db(S, ref=np.max)
    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('é¢‘è°±å›¾')
    
    # 2. æ³¢å½¢å›¾
    ax2 = plt.subplot(3, 2, 2)
    time = np.linspace(0, len(y)/sr, len(y))
    plt.plot(time, y, color='blue', alpha=0.7, linewidth=0.5)
    plt.fill_between(time, y, 0, alpha=0.3, color='blue')
    plt.title('æ³¢å½¢å›¾')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('æŒ¯å¹…')
    plt.grid(True, alpha=0.3)
    
    # 3. é¢‘è°±å›¾ï¼ˆçº¿æ€§é¢‘ç‡ï¼‰
    ax3 = plt.subplot(3, 2, 3)
    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')
    plt.colorbar(format='%+2.0f dB')
    plt.title('é¢‘è°±å›¾ï¼ˆå¯¹æ•°é¢‘ç‡ï¼‰')
    
    # 4. åŸºé¢‘è½¨è¿¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if detected_info.get('frequency'):
        ax4 = plt.subplot(3, 2, 4)
        
        # è®¡ç®—åŸºé¢‘è½¨è¿¹
        f0, voiced_flag, voiced_probs = librosa.pyin(
            y, 
            fmin=librosa.note_to_hz('C2'), 
            fmax=librosa.note_to_hz('C7'),
            sr=sr
        )
        
        times = librosa.times_like(f0, sr=sr)
        
        plt.plot(times, f0, label='åŸºé¢‘è½¨è¿¹', color='red', linewidth=2)
        plt.axhline(y=detected_info['frequency'], color='green', linestyle='--', 
                   label=f"æ£€æµ‹é¢‘ç‡: {detected_info['frequency']:.1f} Hz")
        plt.title('åŸºé¢‘è½¨è¿¹')
        plt.xlabel('æ—¶é—´ (ç§’)')
        plt.ylabel('é¢‘ç‡ (Hz)')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # 5. é¢‘è°±è´¨å¿ƒ
    ax5 = plt.subplot(3, 2, 5)
    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
    times = librosa.times_like(spectral_centroids, sr=sr)
    plt.plot(times, spectral_centroids, color='purple')
    plt.title('é¢‘è°±è´¨å¿ƒ')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('é¢‘ç‡ (Hz)')
    plt.grid(True, alpha=0.3)
    
    # 6. è¿‡é›¶ç‡
    ax6 = plt.subplot(3, 2, 6)
    zero_crossings = librosa.feature.zero_crossing_rate(y)[0]
    times = librosa.times_like(zero_crossings, sr=sr)
    plt.plot(times, zero_crossings, color='orange')
    plt.title('è¿‡é›¶ç‡')
    plt.xlabel('æ—¶é—´ (ç§’)')
    plt.ylabel('è¿‡é›¶ç‡')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

# ========= éŸ³é¢‘æ–‡ä»¶ç®¡ç† =========

def convert_to_serializable(obj):
    """å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_to_serializable(item) for item in obj)
    else:
        return obj

class AudioClipManager:
    """éŸ³é¢‘ç‰‡æ®µç®¡ç†å™¨"""
    
    def __init__(self):
        self.clips = []
        self.load_clips()
    
    def load_clips(self):
        """åŠ è½½å·²ä¿å­˜çš„ç‰‡æ®µ"""
        if os.path.exists('clips.json'):
            with open('clips.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.clips = data.get('clips', [])
    
    def save_clips(self):
        """ä¿å­˜ç‰‡æ®µä¿¡æ¯"""
        # è½¬æ¢æ‰€æœ‰æ•°æ®ä¸ºå¯åºåˆ—åŒ–çš„PythonåŸç”Ÿç±»å‹
        clips_serializable = convert_to_serializable(self.clips)
        data = {'clips': clips_serializable}
        with open('clips.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def add_clip(self, audio_data: np.ndarray, sr: int, 
                 note_info: Dict = None, metadata: Dict = None) -> str:
        """æ·»åŠ éŸ³é¢‘ç‰‡æ®µ"""
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"clip_{timestamp}.wav"
        filepath = os.path.join(config.clip_dir, filename)
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        sf.write(filepath, audio_data, sr)
        
        # å¦‚æœæ²¡æœ‰æä¾›éŸ³é«˜ä¿¡æ¯ï¼Œåˆ™æ£€æµ‹
        if note_info is None:
            note_info = detect_pitch_advanced(audio_data, sr)
        
        # åˆ›å»ºç‰‡æ®µä¿¡æ¯
        clip_info = {
            'id': int(len(self.clips)),
            'filename': filename,
            'filepath': filepath,
            'sample_rate': int(sr),
            'duration': float(len(audio_data) / sr),
            'note_info': convert_to_serializable(note_info) if note_info else {},
            'metadata': convert_to_serializable(metadata) if metadata else {},
            'created_at': timestamp,
            'tags': []
        }
        
        self.clips.append(clip_info)
        self.save_clips()
        clear_clip_index_cache()
        return clip_info
    
    def get_clip_by_note(self, target_note: str, tolerance_cents: float = 50) -> List[Dict]:
        """æ ¹æ®éŸ³åè·å–ç‰‡æ®µ"""
        target_midi = note_to_midi(target_note)
        if target_midi is None:
            return []
        
        matching_clips = []
        for clip in self.clips:
            note_info = clip.get('note_info', {})
            if note_info.get('note') and note_info.get('frequency'):
                clip_midi = freq_to_midi(note_info['frequency'])
                cents_diff = (clip_midi - target_midi) * 100
                
                if abs(cents_diff) <= tolerance_cents:
                    matching_clips.append({
                        **clip,
                        'cents_diff': cents_diff
                    })
        
        # æŒ‰åå·®æ’åº
        matching_clips.sort(key=lambda x: abs(x['cents_diff']))
        return matching_clips
    
    def get_all_clips(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ç‰‡æ®µ"""
        return self.clips
    
    def delete_clip(self, clip_id: int) -> bool:
        """åˆ é™¤ç‰‡æ®µï¼ŒåŒ…æ‹¬jsonè®°å½•å’ŒéŸ³é¢‘æ–‡ä»¶"""
        if 0 <= clip_id < len(self.clips):
            clip = self.clips.pop(clip_id)
            
            # å°è¯•åˆ é™¤éŸ³é¢‘æ–‡ä»¶
            try:
                if os.path.exists(clip['filepath']):
                    os.remove(clip['filepath'])
                    print(f"å·²åˆ é™¤éŸ³é¢‘æ–‡ä»¶: {clip['filepath']}")
            except Exception as e:
                print(f"åˆ é™¤éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™ {clip['filepath']}: {e}")
            
            # æ›´æ–°ID
            for i, c in enumerate(self.clips):
                c['id'] = i
            
            self.save_clips()
            clear_clip_index_cache()
            return True
        return False
    
    def cleanup_orphaned_files(self):
        """æ¸…ç†æ²¡æœ‰å¯¹åº”è®°å½•çš„éŸ³é¢‘æ–‡ä»¶"""
        # è·å–æ‰€æœ‰åº”è¯¥å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„
        expected_files = {clip['filepath'] for clip in self.clips}
        
        # éå†clipsç›®å½•
        clips_dir = config.clip_dir
        for filename in os.listdir(clips_dir):
            if filename.endswith('.wav'):
                filepath = os.path.join(clips_dir, filename)
                if filepath not in expected_files:
                    try:
                        os.remove(filepath)
                        print(f"æ¸…ç†å­¤ç«‹æ–‡ä»¶: {filename}")
                    except Exception as e:
                        print(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {filename}: {e}")

    def delete_all_clips(self):
        """åˆ é™¤æ‰€æœ‰ç‰‡æ®µå’Œå¯¹åº”çš„æ–‡ä»¶"""
        deleted_files = []
        for clip in self.clips:
            try:
                if os.path.exists(clip['filepath']):
                    os.remove(clip['filepath'])
                    deleted_files.append(clip['filename'])
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {clip['filename']}: {e}")
        
        self.clips = []
        self.save_clips()
        clear_clip_index_cache()
        return deleted_files

# ========= Gradioç•Œé¢å‡½æ•° =========

clip_manager = AudioClipManager()

def handle_audio_upload(audio_input, target_note, auto_detect, analysis_mode):
    """å¤„ç†éŸ³é¢‘ä¸Šä¼ """
    if audio_input is None:
        return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", None, None, None
    
    # è¯»å–éŸ³é¢‘
    if isinstance(audio_input, tuple):
        sr, y = audio_input
        y = np.array(y, dtype=np.float32)
    elif isinstance(audio_input, dict):
        sr, y = audio_input["sample_rate"], np.array(audio_input["data"], dtype=np.float32)
    elif isinstance(audio_input, str):
        y, sr = sf.read(audio_input)
        if y.ndim > 1:
            y = np.mean(y, axis=1)
    else:
        return "ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼", None, None, None
    
    # æ£€æµ‹éŸ³é«˜
    note_info = detect_pitch_advanced(y, sr)
    
    message = []
    
    if note_info['frequency']:
        message.append(f"æ£€æµ‹åˆ°é¢‘ç‡: **{note_info['frequency']:.1f} Hz**")
        message.append(f"éŸ³å: **{note_info['note']}**")
        message.append(f"éŸ³åˆ†åå·®: **{note_info['cents']:+.1f} cents**")
        message.append(f"ç½®ä¿¡åº¦: **{note_info['confidence']:.2%}**")
        
        if note_info['stable']:
            message.append("âœ… éŸ³é«˜ç¨³å®š")
        else:
            message.append("âš  éŸ³é«˜ä¸ç¨³å®šï¼Œå¯èƒ½åŒ…å«æ»‘éŸ³æˆ–å¤šéŸ³")
    else:
        message.append("âš  æ— æ³•æ£€æµ‹åˆ°ç¨³å®šéŸ³é«˜")
    
    # å¦‚æœæœ‰ç›®æ ‡éŸ³é«˜ï¼Œè¿›è¡Œæ¯”è¾ƒ
    if target_note:
        target_midi = note_to_midi(target_note)
        if target_midi:
            target_freq = midi_to_freq(target_midi)
            if note_info['frequency']:
                cents_diff = (freq_to_midi(note_info['frequency']) - target_midi) * 100
                message.append(f"ç›®æ ‡éŸ³é«˜: **{target_note}** ({target_freq:.1f} Hz)")
                message.append(f"åå·®: **{cents_diff:+.1f} cents**")
                
                if abs(cents_diff) <= 50:
                    message.append("âœ… åœ¨å¯æ¥å—èŒƒå›´å†… (Â±50 cents)")
                else:
                    message.append("âš  åå·®è¾ƒå¤§")
        else:
            message.append(f"âš  ç›®æ ‡éŸ³é«˜ '{target_note}' æ ¼å¼é”™è¯¯")
    
    # ä¿å­˜ç‰‡æ®µ
    clip_info = clip_manager.add_clip(
        y, sr, 
        note_info=convert_to_serializable(note_info) if note_info else None,
        metadata={
            'target_note': str(target_note) if target_note else "",
            'upload_time': str(time.strftime("%Y-%m-%d %H:%M:%S"))
        }
    )
    
    # ç”Ÿæˆå›¾è¡¨
    if analysis_mode == "simple":
        fig = create_spectrogram(y, sr, note_info.get('frequency'))
        fig2 = None
    else:
        fig = create_spectrogram(y, sr, note_info.get('frequency'))
        fig2 = create_enhanced_analysis(y, sr, note_info)
    
    return "\n".join(message), clip_info['id'], fig, fig2

def process_audio_clip(clip_id, operation, value):
    """å¤„ç†éŸ³é¢‘ç‰‡æ®µï¼ˆå˜é€Ÿ/å˜è°ƒï¼‰"""
    if not 0 <= clip_id < len(clip_manager.clips):
        return "æ— æ•ˆçš„ç‰‡æ®µID", None
    
    clip = clip_manager.clips[clip_id]
    y, sr = sf.read(clip['filepath'])
    if y.ndim > 1:
        y = np.mean(y, axis=1)
    
    if operation == "time_stretch":
        target_duration = float(value)
        y_processed = time_stretch(y, sr, target_duration)
        message = f"æ—¶é•¿è°ƒæ•´ä¸º {target_duration:.2f} ç§’"
    elif operation == "pitch_shift":
        semitones = float(value)
        y_processed = pitch_shift(y, sr, semitones)
        message = f"éŸ³é«˜è°ƒæ•´ {semitones:+.1f} ä¸ªåŠéŸ³"
    else:
        return "æœªçŸ¥æ“ä½œ", None
    
    # åº”ç”¨æ·¡å…¥æ·¡å‡º
    y_processed = apply_fade(y_processed, sr)
    
    # ä¿å­˜å¤„ç†åçš„éŸ³é¢‘
    processed_info = clip_manager.add_clip(
        y_processed, sr,
        metadata={
            'original_clip_id': clip_id,
            'operation': operation,
            'value': value,
            'processed_time': time.strftime("%Y-%m-%d %H:%M:%S")
        }
    )
    
    return f"âœ… {message} (æ–°ç‰‡æ®µID: {processed_info['id']})", (sr, y_processed)

def generate_music_from_clips(clip_assignments, tempo):
    """ä»ç‰‡æ®µç”ŸæˆéŸ³ä¹"""
    sr = config.sample_rate
    beat_duration = 60.0 / tempo
    
    # è§£æç‰‡æ®µåˆ†é…ï¼ˆæ ¼å¼: "æ—¶é—´æ‹:ç‰‡æ®µID,æ—¶é—´æ‹:ç‰‡æ®µID,..."ï¼‰
    assignments = []
    for assignment in clip_assignments.split(','):
        if ':' in assignment:
            beat_str, clip_id_str = assignment.split(':')
            try:
                beat = float(beat_str.strip())
                clip_id = int(clip_id_str.strip())
                assignments.append((beat, clip_id))
            except:
                continue
    
    if not assignments:
        return "æ²¡æœ‰æœ‰æ•ˆçš„ç‰‡æ®µåˆ†é…", None
    
    # æŒ‰æ—¶é—´æ’åº
    assignments.sort(key=lambda x: x[0])
    
    # è®¡ç®—æ€»æ—¶é•¿
    last_beat = max([a[0] for a in assignments]) + 4  # å‡è®¾æ¯ä¸ªç‰‡æ®µ4æ‹
    total_samples = int(last_beat * beat_duration * sr)
    
    # åˆ›å»ºéŸ³è½¨
    track = np.zeros(total_samples)
    
    for beat, clip_id in assignments:
        if 0 <= clip_id < len(clip_manager.clips):
            clip = clip_manager.clips[clip_id]
            y, _ = sf.read(clip['filepath'])
            if y.ndim > 1:
                y = np.mean(y, axis=1)
            
            # è°ƒæ•´åˆ°æ ‡å‡†æ—¶é•¿ï¼ˆ1æ‹ï¼‰
            target_samples = int(beat_duration * sr)
            if len(y) > target_samples:
                y = y[:target_samples]
            else:
                y = np.pad(y, (0, target_samples - len(y)), mode='constant')
            
            # æ·»åŠ åˆ°éŸ³è½¨
            start_sample = int(beat * beat_duration * sr)
            end_sample = start_sample + len(y)
            
            if end_sample <= len(track):
                track[start_sample:end_sample] += y
    
    # å½’ä¸€åŒ–
    track = normalize_audio(track)
    
    # ä¿å­˜ç»“æœ
    output_filename = f"composition_{time.strftime('%Y%m%d_%H%M%S')}.wav"
    output_path = os.path.join(config.output_dir, output_filename)
    sf.write(output_path, track, sr)
    
    return f"âœ… éŸ³ä¹ç”Ÿæˆå®Œæˆ: {output_filename}", (sr, track)

# ========= éŸ³ä¹ç”Ÿæˆç•Œé¢ ===========
def build_music_composition_tab():
    """æ„å»ºå…¨æ–°çš„è‡ªåŠ¨éŸ³ä¹åˆ¶ä½œç•Œé¢"""
    
    with gr.TabItem("ğŸ¹ æ™ºèƒ½éŸ³ä¹åˆ¶ä½œ"):
        gr.Markdown("""
        ## ğŸ¼ æ™ºèƒ½éŸ³ä¹åˆ¶ä½œå·¥ä½œå°
        ä¸Šä¼ ä¹è°± â†’ è‡ªåŠ¨åŒ¹é…éŸ³é¢‘ç‰‡æ®µ â†’ æ™ºèƒ½å˜è°ƒå¤„ç† â†’ ç”Ÿæˆå®Œæ•´éŸ³ä¹
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # ä¹è°±ä¸Šä¼ åŒºåŸŸ
                gr.Markdown("### 1. ä¸Šä¼ ä¹è°±")
                score_upload = gr.File(
                    label="é€‰æ‹©ä¹è°±æ–‡ä»¶",
                    file_types=[".xml", ".musicxml", ".mid", ".midi"],
                    type="filepath"
                )
                
                # ä¹è°±ä¿¡æ¯å±•ç¤º
                score_info = gr.Markdown("ç­‰å¾…ä¸Šä¼ ä¹è°±...", label="ä¹è°±ä¿¡æ¯")
                
                # å¤„ç†é€‰é¡¹
                gr.Markdown("### 2. å¤„ç†é€‰é¡¹")
                
                with gr.Row():
                    match_tolerance = gr.Slider(
                        minimum=0,
                        maximum=100,
                        value=20,
                        step=5,
                        label="éŸ³é«˜åŒ¹é…å®¹å·® (cents)",
                        info="å€¼è¶Šå°åŒ¹é…è¦æ±‚è¶Šä¸¥æ ¼"
                    )
                    
                    use_pitch_shift = gr.Checkbox(
                        label="å¯ç”¨æ™ºèƒ½å˜è°ƒ",
                        value=True,
                        info="å¯¹ä¸åŒ¹é…çš„éŸ³ç¬¦è‡ªåŠ¨å˜è°ƒå¤„ç†"
                    )
                
                tempo_input = gr.Slider(
                    label="æ¼”å¥é€Ÿåº¦ (BPM)",
                    minimum=40,
                    maximum=240,
                    value=120,
                    step=5
                )
                
                # ç”ŸæˆæŒ‰é’®
                btn_generate = gr.Button("ğŸµ è‡ªåŠ¨ç”ŸæˆéŸ³ä¹", variant="primary", size="lg")
                generation_status = gr.Markdown("å‡†å¤‡ç”Ÿæˆ...", label="ç”ŸæˆçŠ¶æ€")
                
            with gr.Column(scale=2):
                # ç”Ÿæˆç»“æœåŒºåŸŸ
                gr.Markdown("### 3. ç”Ÿæˆç»“æœ")
                
                with gr.Tabs():
                    with gr.TabItem("ğŸ§ è¯•å¬éŸ³ä¹"):
                        composition_audio = gr.Audio(
                            label="ç”ŸæˆéŸ³ä¹",
                            type="numpy"
                        )
                    
                    with gr.TabItem("ğŸ“Š ç”ŸæˆæŠ¥å‘Š"):
                        generation_report = gr.Markdown(
                            "ç”ŸæˆæŠ¥å‘Šå°†åœ¨æ­¤æ˜¾ç¤º...",
                            label="è¯¦ç»†æŠ¥å‘Š"
                        )
                    
                    with gr.TabItem("ğŸµ éŸ³ç¬¦åŒ¹é…è¯¦æƒ…"):
                        notes_match_table = gr.Dataframe(
                            headers=["åºå·", "éŸ³å", "åŒ¹é…ç‰‡æ®µ", "å˜è°ƒ(åŠéŸ³)", "çŠ¶æ€", "éŸ³è½¨", "ä¹å™¨"],
                            label="éŸ³ç¬¦åŒ¹é…æƒ…å†µ",
                            datatype=["str", "str", "str", "str", "str", "str", "str"],
                            row_count=10,
                            interactive=False
                        )
        
        # è¿æ¥ç”ŸæˆæŒ‰é’®
        btn_generate.click(
            fn=auto_generate_music_from_score,
            inputs=[score_upload, tempo_input, match_tolerance, use_pitch_shift],
            outputs=[composition_audio, generation_report, notes_match_table, generation_status]
        )
        
        # ä¹è°±ä¸Šä¼ åçš„é¢„è§ˆ
        def preview_score(filepath):
            if filepath is None:
                return "ç­‰å¾…ä¸Šä¼ ä¹è°±...", []
            
            try:
                notes = parse_score_notes(filepath)
                if not notes:
                    return "æœªèƒ½ä»ä¹è°±ä¸­è§£æå‡ºéŸ³ç¬¦", []
                
                # æ„å»ºé¢„è§ˆä¿¡æ¯
                preview_text = f"### ä¹è°±è§£ææˆåŠŸï¼\n"
                preview_text += f"**éŸ³ç¬¦æ€»æ•°**: {len(notes)}\n"
                preview_text += f"**éŸ³é«˜èŒƒå›´**: {notes[0]['note_name']} åˆ° {notes[-1]['note_name']}\n"
                preview_text += f"**æ€»æ—¶é•¿**: {sum(n['duration'] for n in notes):.2f} æ‹\n\n"
                preview_text += "**å‰10ä¸ªéŸ³ç¬¦**:\n"
                
                # æ„å»ºè¡¨æ ¼æ•°æ®
                table_data = []
                for i, note in enumerate(notes[:10]):
                    table_data.append([
                        i+1,
                        note['note_name'],
                        f"{note['duration']:.2f}æ‹",
                        f"{note['start_time']:.2f}æ‹",
                        "æ˜¯" if note['matched'] else "å¦"
                    ])
                
                preview_text += "(è¯¦ç»†åŒ¹é…æƒ…å†µå°†åœ¨ç”Ÿæˆæ—¶æ˜¾ç¤º)"
                return preview_text, table_data
                
            except Exception as e:
                return f"è§£æä¹è°±æ—¶å‡ºé”™: {str(e)}", []
        
        score_upload.change(
            fn=preview_score,
            inputs=[score_upload],
            outputs=[score_info, notes_match_table]
        )

# ========= æ ¸å¿ƒéŸ³ä¹ç”Ÿæˆå‡½æ•° =========

# ========= æ ¸å¿ƒéŸ³ä¹ç”Ÿæˆå‡½æ•° =========

def parse_score_notes(filepath: str) -> List[Dict]:
    """
    ä¸“ä¸šè§£æä¹è°±æ–‡ä»¶ï¼Œæå–éŸ³ç¬¦ä¿¡æ¯
    æ”¯æŒ MusicXML å’Œ MIDI æ ¼å¼ï¼ŒMIDIè§£æç°åœ¨æ”¯æŒå®Œæ•´éŸ³ç¬¦è¿½è¸ªã€å¤šéŸ³è½¨ã€ä¼‘æ­¢ç¬¦æ£€æµ‹
    """
    notes = []
    
    if not filepath or not os.path.exists(filepath):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„é”™è¯¯: {filepath}")
        return notes
    
    file_ext = os.path.splitext(filepath)[1].lower()
    
    try:
        if file_ext in ['.xml', '.musicxml']:
            # ============ MusicXML è§£æéƒ¨åˆ† ============
            # å°è¯•ä½¿ç”¨ partitura è§£æï¼ˆæ›´ä¸“ä¸šï¼‰
            try:
                import partitura as pt
                score = pt.load_score(filepath)
                print(f"ä½¿ç”¨ partitura è§£æ XMLï¼Œå…±æ‰¾åˆ° {len(score.notes)} ä¸ªéŸ³ç¬¦")
                
                for i, note in enumerate(score.notes):
                    notes.append({
                        'midi_pitch': int(note.midi_pitch),
                        'note_name': note.step + str(note.octave),
                        'duration': float(note.duration),
                        'start_time': float(note.start),
                        'velocity': int(note.velocity) if hasattr(note, 'velocity') else 64,
                        'matched': False,
                        'clip_id': None,
                        'pitch_shift': 0,
                        'track': 0,  # XMLé€šå¸¸ä¸åˆ†è½¨
                        'instrument': 'piano',  # é»˜è®¤
                        'source': 'xml'
                    })
                    
            except ImportError:
                print("æœªå®‰è£… partituraï¼Œä½¿ç”¨ music21 è§£æ XML")
                # å›é€€åˆ° music21
                import music21 as m21
                score = m21.converter.parse(filepath)
                
                # è·å–æ‰€æœ‰éŸ³ç¬¦
                all_notes = list(score.flat.notesAndRests)
                print(f"ä½¿ç”¨ music21 è§£æ XMLï¼Œå…±æ‰¾åˆ° {len(all_notes)} ä¸ªéŸ³ç¬¦/ä¼‘æ­¢ç¬¦")
                
                for element in all_notes:
                    if isinstance(element, m21.note.Note):
                        notes.append({
                            'midi_pitch': element.pitch.midi,
                            'note_name': str(element.pitch),
                            'duration': float(element.duration.quarterLength),
                            'start_time': float(element.offset),
                            'velocity': 64,
                            'matched': False,
                            'clip_id': None,
                            'pitch_shift': 0,
                            'track': 0,
                            'instrument': 'piano',
                            'source': 'xml'
                        })
                    elif isinstance(element, m21.note.Rest):
                        # å°†ä¼‘æ­¢ç¬¦è®°å½•ä¸ºç‰¹æ®ŠéŸ³ç¬¦ï¼Œmidi_pitchä¸º-1
                        notes.append({
                            'midi_pitch': -1,  # ä¼‘æ­¢ç¬¦æ ‡è¯†
                            'note_name': 'REST',
                            'duration': float(element.duration.quarterLength),
                            'start_time': float(element.offset),
                            'velocity': 0,
                            'matched': False,
                            'clip_id': None,
                            'pitch_shift': 0,
                            'track': 0,
                            'instrument': 'rest',
                            'source': 'xml'
                        })
        
        elif file_ext in ['.mid', '.midi']:
            # ============ ä¸“ä¸šMIDIè§£æéƒ¨åˆ† ============
            import mido
            
            print(f"å¼€å§‹è§£æ MIDI æ–‡ä»¶: {os.path.basename(filepath)}")
            midi = mido.MidiFile(filepath)
            
            # è·å–MIDIæ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯
            ticks_per_beat = midi.ticks_per_beat
            print(f"MIDIåŸºæœ¬ä¿¡æ¯ - éŸ³è½¨æ•°: {len(midi.tracks)}, æ¯æ‹Tickæ•°: {ticks_per_beat}, ç±»å‹: {midi.type}")
            
            # å­˜å‚¨å„éŸ³è½¨çš„å½“å‰æ—¶é—´å’Œæ´»åŠ¨éŸ³ç¬¦
            track_info = []
            for i in range(len(midi.tracks)):
                track_info.append({
                    'current_time': 0,  # å½“å‰ç»å¯¹æ—¶é—´ï¼ˆtickï¼‰
                    'active_notes': {},  # æ­£åœ¨æ’­æ”¾çš„éŸ³ç¬¦: {note_number: start_tick}
                    'tempo': 500000,  # é»˜è®¤tempo (120 BPM)
                    'time_signature': (4, 4),  # é»˜è®¤æ‹å·
                    'key_signature': 'C',  # é»˜è®¤è°ƒå·
                    'program': 0,  # é»˜è®¤ä¹å™¨ (Acoustic Grand Piano)
                })
            
            # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰éŸ³ç¬¦å¼€å§‹å’Œç»“æŸäº‹ä»¶
            note_events = []  # (absolute_tick, track_index, note_number, velocity, event_type)
            
            for track_idx, track in enumerate(midi.tracks):
                current_tick = 0
                
                print(f"  è§£æéŸ³è½¨ {track_idx}: {track.name if track.name else 'æœªå‘½å'}, æ¶ˆæ¯æ•°: {len(track)}")
                
                for msg in track:
                    current_tick += msg.time
                    
                    if msg.type == 'note_on':
                        if msg.velocity > 0:
                            # éŸ³ç¬¦å¼€å§‹
                            note_events.append((current_tick, track_idx, msg.note, msg.velocity, 'start'))
                        else:
                            # velocity=0 çš„ note_on ç­‰ä»·äº note_off
                            note_events.append((current_tick, track_idx, msg.note, 0, 'end'))
                    
                    elif msg.type == 'note_off':
                        # éŸ³ç¬¦ç»“æŸ
                        note_events.append((current_tick, track_idx, msg.note, 0, 'end'))
                    
                    elif msg.type == 'set_tempo':
                        # è®°å½•é€Ÿåº¦å˜åŒ– (å¾®ç§’æ¯æ‹)
                        track_info[track_idx]['tempo'] = msg.tempo
                    
                    elif msg.type == 'time_signature':
                        # è®°å½•æ‹å·å˜åŒ–
                        track_info[track_idx]['time_signature'] = (msg.numerator, msg.denominator)
                    
                    elif msg.type == 'key_signature':
                        # è®°å½•è°ƒå·å˜åŒ–
                        track_info[track_idx]['key_signature'] = msg.key
                    
                    elif msg.type == 'program_change':
                        # è®°å½•ä¹å™¨å˜åŒ–
                        track_info[track_idx]['program'] = msg.program
            
            # æŒ‰æ—¶é—´æ’åºæ‰€æœ‰äº‹ä»¶
            note_events.sort(key=lambda x: x[0])
            
            # ç¬¬äºŒéï¼šåŒ¹é…éŸ³ç¬¦çš„å¼€å§‹å’Œç»“æŸï¼Œè®¡ç®—æ—¶é•¿
            active_notes_map = {}  # (track_idx, note_number) -> start_tick
            
            for event in note_events:
                abs_tick, track_idx, note_num, velocity, event_type = event
                key = (track_idx, note_num)
                
                if event_type == 'start':
                    # è®°å½•éŸ³ç¬¦å¼€å§‹
                    active_notes_map[key] = {
                        'start_tick': abs_tick,
                        'velocity': velocity,
                        'track_idx': track_idx
                    }
                elif event_type == 'end' and key in active_notes_map:
                    # æ‰¾åˆ°åŒ¹é…çš„éŸ³ç¬¦ç»“æŸï¼Œè®¡ç®—æ—¶é•¿
                    start_info = active_notes_map.pop(key)
                    duration_ticks = abs_tick - start_info['start_tick']
                    
                    if duration_ticks > 0:  # è¿‡æ»¤æ‰æ—¶é•¿ä¸º0çš„éŸ³ç¬¦
                        # å°†tickè½¬æ¢ä¸ºæ‹æ•° (beats)
                        duration_beats = duration_ticks / ticks_per_beat
                        start_beats = start_info['start_tick'] / ticks_per_beat
                        
                        # è·å–å½“å‰éŸ³è½¨ä¿¡æ¯
                        track_data = track_info[track_idx]
                        
                        # è®¡ç®—BPM
                        bpm = 60_000_000 / track_data['tempo']  # å¾®ç§’è½¬BPM
                        
                        # æ ¹æ®ä¹å™¨programè·å–ä¹å™¨åç§°
                        instrument_name = get_instrument_name(track_data['program'])
                        
                        notes.append({
                            'midi_pitch': note_num,
                            'note_name': midi_to_note(note_num),
                            'duration': float(duration_beats),
                            'start_time': float(start_beats),
                            'velocity': start_info['velocity'],
                            'matched': False,
                            'clip_id': None,
                            'pitch_shift': 0,
                            'track': track_idx,
                            'instrument': instrument_name,
                            'program': track_data['program'],
                            'tempo': bpm,
                            'time_signature': track_data['time_signature'],
                            'key_signature': track_data['key_signature'],
                            'source': 'midi'
                        })
            
            # å¤„ç†æœªç»“æŸçš„éŸ³ç¬¦ï¼ˆå¦‚æœMIDIæ–‡ä»¶æ²¡æœ‰ç›¸åº”çš„note_offï¼‰
            for key, start_info in active_notes_map.items():
                track_idx, note_num = key
                # å‡è®¾éŸ³ç¬¦æŒç»­åˆ°æ–‡ä»¶æœ«å°¾æˆ–ç»™ä¸€ä¸ªé»˜è®¤æ—¶é•¿
                final_tick = max([event[0] for event in note_events]) if note_events else 0
                duration_ticks = final_tick - start_info['start_tick']
                
                if duration_ticks > 0:
                    duration_beats = duration_ticks / ticks_per_beat
                    start_beats = start_info['start_tick'] / ticks_per_beat
                    
                    track_data = track_info[track_idx]
                    instrument_name = get_instrument_name(track_data['program'])
                    
                    notes.append({
                        'midi_pitch': note_num,
                        'note_name': midi_to_note(note_num),
                        'duration': float(duration_beats),
                        'start_time': float(start_beats),
                        'velocity': start_info['velocity'],
                        'matched': False,
                        'clip_id': None,
                        'pitch_shift': 0,
                        'track': track_idx,
                        'instrument': instrument_name,
                        'program': track_data['program'],
                        'tempo': 60_000_000 / track_data['tempo'],
                        'time_signature': track_data['time_signature'],
                        'key_signature': track_data['key_signature'],
                        'source': 'midi'
                    })
            
            print(f"MIDIè§£æå®Œæˆï¼Œå…±æå– {len(notes)} ä¸ªéŸ³ç¬¦")
            
            # æ£€æµ‹å¹¶æ·»åŠ ä¼‘æ­¢ç¬¦
            notes = add_rests_to_midi(notes)
    
    except Exception as e:
        print(f"è§£æä¹è°±å¤±è´¥ {filepath}: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # è¿”å›ç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•ï¼ˆä»…å½“å®Œå…¨å¤±è´¥æ—¶ï¼‰
        notes = [
            {'midi_pitch': 60, 'note_name': 'C4', 'duration': 1.0, 'start_time': 0.0, 
             'velocity': 64, 'matched': False, 'clip_id': None, 'pitch_shift': 0,
             'track': 0, 'instrument': 'piano', 'source': 'fallback'},
            {'midi_pitch': 62, 'note_name': 'D4', 'duration': 1.0, 'start_time': 1.0, 
             'velocity': 64, 'matched': False, 'clip_id': None, 'pitch_shift': 0,
             'track': 0, 'instrument': 'piano', 'source': 'fallback'},
            {'midi_pitch': 64, 'note_name': 'E4', 'duration': 2.0, 'start_time': 2.0, 
             'velocity': 64, 'matched': False, 'clip_id': None, 'pitch_shift': 0,
             'track': 0, 'instrument': 'piano', 'source': 'fallback'},
        ]
    
    # æŒ‰å¼€å§‹æ—¶é—´æ’åº
    notes.sort(key=lambda x: x['start_time'])
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if notes:
        print(f"è§£æç»Ÿè®¡: å…± {len(notes)} ä¸ªéŸ³ç¬¦")
        print(f"éŸ³é«˜èŒƒå›´: {min(n['midi_pitch'] for n in notes if n['midi_pitch'] > 0)} åˆ° {max(n['midi_pitch'] for n in notes)}")
        print(f"æ—¶é—´èŒƒå›´: {notes[0]['start_time']:.2f} åˆ° {notes[-1]['start_time'] + notes[-1]['duration']:.2f} æ‹")
        
        # æŒ‰éŸ³è½¨ç»Ÿè®¡
        if any(n['track'] > 0 for n in notes):
            tracks = set(n['track'] for n in notes)
            print(f"éŸ³è½¨æ•°: {len(tracks)}")
    
    return notes

def get_instrument_name(program: int) -> str:
    """æ ¹æ®MIDIç¨‹åºå·è·å–ä¹å™¨åç§°"""
    # GM (General MIDI) ä¹å™¨åˆ—è¡¨ (0-127)
    gm_instruments = [
        "Acoustic Grand Piano", "Bright Acoustic Piano", "Electric Grand Piano", 
        "Honky-tonk Piano", "Electric Piano 1", "Electric Piano 2", "Harpsichord", 
        "Clavinet", "Celesta", "Glockenspiel", "Music Box", "Vibraphone", 
        "Marimba", "Xylophone", "Tubular Bells", "Dulcimer", "Drawbar Organ", 
        "Percussive Organ", "Rock Organ", "Church Organ", "Reed Organ", 
        "Accordion", "Harmonica", "Tango Accordion", "Acoustic Guitar (nylon)", 
        "Acoustic Guitar (steel)", "Electric Guitar (jazz)", "Electric Guitar (clean)", 
        "Electric Guitar (muted)", "Overdriven Guitar", "Distortion Guitar", 
        "Guitar harmonics", "Acoustic Bass", "Electric Bass (finger)", 
        "Electric Bass (pick)", "Fretless Bass", "Slap Bass 1", "Slap Bass 2", 
        "Synth Bass 1", "Synth Bass 2", "Violin", "Viola", "Cello", "Contrabass", 
        "Tremolo Strings", "Pizzicato Strings", "Orchestral Harp", "Timpani", 
        "String Ensemble 1", "String Ensemble 2", "Synth Strings 1", "Synth Strings 2", 
        "Choir Aahs", "Voice Oohs", "Synth Voice", "Orchestra Hit", "Trumpet", 
        "Trombone", "Tuba", "Muted Trumpet", "French Horn", "Brass Section", 
        "Synth Brass 1", "Synth Brass 2", "Soprano Sax", "Alto Sax", "Tenor Sax", 
        "Baritone Sax", "Oboe", "English Horn", "Bassoon", "Clarinet", "Piccolo", 
        "Flute", "Recorder", "Pan Flute", "Blown Bottle", "Shakuhachi", "Whistle", 
        "Ocarina", "Lead 1 (square)", "Lead 2 (sawtooth)", "Lead 3 (calliope)", 
        "Lead 4 (chiff)", "Lead 5 (charang)", "Lead 6 (voice)", "Lead 7 (fifths)", 
        "Lead 8 (bass + lead)", "Pad 1 (new age)", "Pad 2 (warm)", "Pad 3 (polysynth)", 
        "Pad 4 (choir)", "Pad 5 (bowed)", "Pad 6 (metallic)", "Pad 7 (halo)", 
        "Pad 8 (sweep)", "FX 1 (rain)", "FX 2 (soundtrack)", "FX 3 (crystal)", 
        "FX 4 (atmosphere)", "FX 5 (brightness)", "FX 6 (goblins)", "FX 7 (echoes)", 
        "FX 8 (sci-fi)", "Sitar", "Banjo", "Shamisen", "Koto", "Kalimba", 
        "Bag pipe", "Fiddle", "Shanai", "Tinkle Bell", "Agogo", "Steel Drums", 
        "Woodblock", "Taiko Drum", "Melodic Tom", "Synth Drum", "Reverse Cymbal", 
        "Guitar Fret Noise", "Breath Noise", "Seashore", "Bird Tweet", 
        "Telephone Ring", "Helicopter", "Applause", "Gunshot"
    ]
    
    if 0 <= program < 128:
        return gm_instruments[program]
    return f"Unknown ({program})"

def add_rests_to_midi(notes: List[Dict]) -> List[Dict]:
    """åœ¨MIDIéŸ³ç¬¦ä¹‹é—´æ£€æµ‹å¹¶æ·»åŠ ä¼‘æ­¢ç¬¦"""
    if not notes:
        return notes
    
    notes_with_rests = []
    notes.sort(key=lambda x: (x['track'], x['start_time']))
    
    # æŒ‰éŸ³è½¨åˆ†ç»„å¤„ç†
    tracks = {}
    for note in notes:
        track_num = note['track']
        if track_num not in tracks:
            tracks[track_num] = []
        tracks[track_num].append(note)
    
    # ä¸ºæ¯ä¸ªéŸ³è½¨æ·»åŠ ä¼‘æ­¢ç¬¦
    for track_num, track_notes in tracks.items():
        track_notes.sort(key=lambda x: x['start_time'])
        
        current_time = 0.0
        
        for note in track_notes:
            # å¦‚æœå½“å‰æ—¶é—´å’ŒéŸ³ç¬¦å¼€å§‹æ—¶é—´æœ‰é—´éš”ï¼Œæ·»åŠ ä¼‘æ­¢ç¬¦
            if note['start_time'] > current_time:
                rest_duration = note['start_time'] - current_time
                
                notes_with_rests.append({
                    'midi_pitch': -1,  # ä¼‘æ­¢ç¬¦æ ‡è¯†
                    'note_name': 'REST',
                    'duration': float(rest_duration),
                    'start_time': float(current_time),
                    'velocity': 0,
                    'matched': False,
                    'clip_id': None,
                    'pitch_shift': 0,
                    'track': track_num,
                    'instrument': 'rest',
                    'program': -1,
                    'tempo': note.get('tempo', 120),
                    'time_signature': note.get('time_signature', (4, 4)),
                    'key_signature': note.get('key_signature', 'C'),
                    'source': 'midi_rest'
                })
            
            notes_with_rests.append(note)
            current_time = note['start_time'] + note['duration']
    
    # é‡æ–°æŒ‰æ—¶é—´æ’åº
    notes_with_rests.sort(key=lambda x: x['start_time'])
    return notes_with_rests


def find_best_match_for_note(target_midi: int, tolerance_cents: float = 50.0, 
                           use_confidence_weight: bool = True) -> Tuple[Optional[Dict], float]:
    """
    ä¸ºç›®æ ‡éŸ³ç¬¦å¯»æ‰¾æœ€ä½³åŒ¹é…çš„éŸ³é¢‘ç‰‡æ®µï¼ˆä¼˜åŒ–ç‰ˆï¼‰ã€‚
    
    å‚æ•°:
        target_midi: ç›®æ ‡MIDIéŸ³é«˜ (æ•´æ•°ï¼Œå¦‚ 60 ä»£è¡¨ C4)
        tolerance_cents: éŸ³é«˜å®¹å·® (éŸ³åˆ†)
        use_confidence_weight: æ˜¯å¦ä½¿ç”¨ç½®ä¿¡åº¦ä½œä¸ºæƒé‡
    
    è¿”å›:
        (æœ€ä½³ç‰‡æ®µä¿¡æ¯, éœ€è¦å˜è°ƒçš„åŠéŸ³æ•°)
    """
    # 1. è·å–æˆ–æ„å»ºç´¢å¼•
    index = build_clip_index()
    if not index:
        return None, 0.0  # æ— å¯ç”¨ç‰‡æ®µ
    
    best_clip = None
    best_semitones = 0.0
    best_score = -float('inf')  # ä½¿ç”¨è¯„åˆ†ç³»ç»Ÿï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¥½
    
    # 2. ç¡®å®šæœç´¢èŒƒå›´ï¼šç›®æ ‡éŸ³é«˜é™„è¿‘ Â± (å®¹å·®/100 + 1) ä¸ªåŠéŸ³
    search_semitones = int(tolerance_cents / 100) + 2
    lower_bound = target_midi - search_semitones
    upper_bound = target_midi + search_semitones
    
    # 3. åœ¨ç´¢å¼•çš„é‚»è¿‘é”®ä¸­æœç´¢
    for search_midi in range(lower_bound, upper_bound + 1):
        if search_midi not in index:
            continue
        
        for clip_data in index[search_midi]:
            clip = clip_data['clip']
            clip_exact_midi = clip_data['exact_midi']
            confidence = clip_data['confidence']
            
            # è®¡ç®—ç²¾ç¡®çš„éŸ³é«˜å·®å¼‚ï¼ˆåŠéŸ³ï¼‰
            semitones_diff = target_midi - clip_exact_midi
            cents_diff = semitones_diff * 100.0
            
            # å¦‚æœåœ¨ç»å¯¹å®¹å·®èŒƒå›´å†…ï¼Œæ‰è€ƒè™‘
            if abs(cents_diff) <= tolerance_cents:
                # è®¡ç®—åŒ¹é…åˆ†æ•°ï¼šéŸ³åˆ†è¶Šæ¥è¿‘ã€ç½®ä¿¡åº¦è¶Šé«˜ï¼Œåˆ†æ•°è¶Šé«˜
                closeness_score = 1.0 - (abs(cents_diff) / tolerance_cents)  # 0åˆ°1
                confidence_score = confidence if use_confidence_weight else 1.0
                
                # ç»¼åˆåˆ†æ•° (å¯ä»¥è°ƒæ•´æƒé‡)
                total_score = (closeness_score * 0.7) + (confidence_score * 0.3)
                
                if total_score > best_score:
                    best_score = total_score
                    best_clip = clip
                    best_semitones = semitones_diff
    
    # 4. å¦‚æœæœªæ‰¾åˆ°å®¹å·®å†…çš„ï¼Œè¿”å›æœ€æ¥è¿‘çš„ï¼ˆåŸé€»è¾‘çš„é™çº§æ–¹æ¡ˆï¼‰
    if best_clip is None:
        # è¿™é‡Œå¯ä»¥ä¿ç•™ä½ åŸæœ‰çš„çº¿æ€§æœç´¢é€»è¾‘ä½œä¸ºfallbackï¼Œä½†ä½¿ç”¨ç´¢å¼•é€šå¸¸èƒ½æ‰¾åˆ°
        print(f"[åŒ¹é…è­¦å‘Š] æœªåœ¨å®¹å·® {tolerance_cents} éŸ³åˆ†å†…æ‰¾åˆ° MIDI {target_midi} çš„åŒ¹é…ï¼Œè¿”å›æœ€æ¥è¿‘çš„ã€‚")
        # ç®€å•å®ç°ï¼šéå†æ‰€æœ‰ç‰‡æ®µæ‰¾æœ€æ¥è¿‘çš„
        available_clips = clip_manager.get_all_clips()
        best_distance = float('inf')
        for clip in available_clips:
            note_info = clip.get('note_info', {})
            if note_info and note_info.get('frequency'):
                clip_freq = note_info['frequency']
                clip_midi = freq_to_midi(clip_freq)
                semitones_diff = target_midi - clip_midi
                cents_diff = abs(semitones_diff * 100)
                if cents_diff < best_distance:
                    best_distance = cents_diff
                    best_clip = clip
                    best_semitones = semitones_diff
    
    return best_clip, best_semitones

# å¯é€‰ï¼šå½“clip_managerçš„ç‰‡æ®µåˆ—è¡¨æ›´æ–°æ—¶ï¼Œæ¸…é™¤ç¼“å­˜ä»¥é‡å»ºç´¢å¼•
def clear_clip_index_cache():
    """å½“æ·»åŠ æˆ–åˆ é™¤éŸ³é¢‘ç‰‡æ®µåï¼Œè°ƒç”¨æ­¤å‡½æ•°æ¸…é™¤ç´¢å¼•ç¼“å­˜"""
    global _clip_index_cache
    _clip_index_cache = None
    print("éŸ³é¢‘ç‰‡æ®µç´¢å¼•ç¼“å­˜å·²æ¸…é™¤ï¼Œå°†åœ¨ä¸‹æ¬¡åŒ¹é…æ—¶é‡å»ºã€‚")

def auto_generate_music_from_score(score_file, tempo=120, tolerance_cents=20.0, use_pitch_shift=True):
    """
    è‡ªåŠ¨ä»ä¹è°±ç”ŸæˆéŸ³ä¹çš„ä¸»å‡½æ•°
    """
    if not score_file:
        return None, "è¯·å…ˆä¸Šä¼ ä¹è°±æ–‡ä»¶", [], "âŒ æœªä¸Šä¼ ä¹è°±"
    
    try:
        generation_status = "ğŸ”„ å¼€å§‹è§£æä¹è°±..."
        yield None, generation_status, [], "è§£æä¸­..."
        
        # 1. è§£æä¹è°±
        notes = parse_score_notes(score_file)
        if not notes:
            return None, "âŒ æœªèƒ½ä»ä¹è°±ä¸­è§£æå‡ºéŸ³ç¬¦", [], "è§£æå¤±è´¥"
        
        generation_status = f"âœ… è§£æå®Œæˆï¼Œå…± {len(notes)} ä¸ªéŸ³ç¬¦\nğŸ”„ å¼€å§‹åŒ¹é…éŸ³é¢‘ç‰‡æ®µ..."
        yield None, generation_status, [], "åŒ¹é…ä¸­..."
        
        # 2. åŒ¹é…éŸ³é¢‘ç‰‡æ®µ
        sr = config.sample_rate
        beat_duration = 60.0 / tempo
        match_details = []
        
        # ä¸ºæ¯ä¸ªéŸ³ç¬¦åŒ¹é…ç‰‡æ®µ
        for i, note in enumerate(notes):
            target_midi = note['midi_pitch']
            
            # >>> ä¿®æ”¹ç‚¹1ï¼šä¼˜å…ˆå¤„ç†ä¼‘æ­¢ç¬¦ <<<
            if target_midi == -1:
                note['matched'] = True
                note['is_rest'] = True
                match_details.append([
                    f"éŸ³ç¬¦{i+1}",
                    note['note_name'],
                    f"ä¼‘æ­¢ç¬¦ ({note['duration']:.2f}æ‹)",
                    "N/A",
                    "â¸ï¸ ä¼‘æ­¢",
                    note.get('track', 0),  # å±•ç¤ºéŸ³è½¨ä¿¡æ¯
                    note.get('instrument', 'rest')
                ])
                continue  # è·³è¿‡åç»­åŒ¹é…é€»è¾‘
            
            # å¯»æ‰¾æœ€ä½³åŒ¹é…ï¼ˆä»…é’ˆå¯¹æ™®é€šéŸ³ç¬¦ï¼‰
            best_clip, semitones_diff = find_best_match_for_note(target_midi, tolerance_cents)
            
            if best_clip:
                note['matched'] = True
                note['clip_id'] = best_clip['id']
                note['pitch_shift'] = semitones_diff if use_pitch_shift else 0
                
                match_status = "âœ… å®Œå…¨åŒ¹é…" if abs(semitones_diff) < 0.1 else f"ğŸ”„ éœ€å˜è°ƒ {semitones_diff:+.1f} åŠéŸ³"
                
                match_details.append([
                    f"éŸ³ç¬¦{i+1}",
                    note['note_name'],
                    f"ç‰‡æ®µ{best_clip['id']} ({best_clip.get('note_info', {}).get('note', 'æœªçŸ¥')})",
                    f"{semitones_diff:+.1f}" if use_pitch_shift else "0",
                    match_status,
                    note.get('track', 0),  # æ–°å¢ï¼šå±•ç¤ºéŸ³è½¨ä¿¡æ¯
                    note.get('instrument', 'unknown')  # æ–°å¢ï¼šå±•ç¤ºä¹å™¨ä¿¡æ¯
                ])
            else:
                note['matched'] = False
                match_details.append([
                    f"éŸ³ç¬¦{i+1}",
                    note['note_name'],
                    "æ— å¯ç”¨ç‰‡æ®µ",
                    "N/A",
                    "âŒ æœªåŒ¹é…",
                    note.get('track', 0),
                    note.get('instrument', 'unknown')
                ])
        
        # ç»Ÿè®¡åŒ¹é…ç»“æœï¼ˆä»…ç»Ÿè®¡æ™®é€šéŸ³ç¬¦ï¼Œæ’é™¤ä¼‘æ­¢ç¬¦ï¼‰
        valid_notes = [n for n in notes if n.get('midi_pitch', 0) != -1]
        matched_count = sum(1 for n in valid_notes if n['matched'])
        total_valid_notes = len(valid_notes)
        match_rate = matched_count / total_valid_notes * 100 if total_valid_notes > 0 else 0
        
        generation_status = f"âœ… åŒ¹é…å®Œæˆ: {matched_count}/{total_valid_notes} ä¸ªå¯åŒ¹é…éŸ³ç¬¦ ({match_rate:.1f}%)\nğŸ”„ å¼€å§‹å¤„ç†éŸ³é¢‘..."
        yield None, generation_status, match_details, "å¤„ç†ä¸­..."
        
        # 3. å¤„ç†éŸ³é¢‘ç‰‡æ®µ
        processed_clips = {}
        audio_segments = []
        
        for i, note in enumerate(notes):
            # >>> ä¿®æ”¹ç‚¹2ï¼šä¼˜å…ˆå¤„ç†ä¼‘æ­¢ç¬¦ <<<
            if note.get('is_rest') or note['midi_pitch'] == -1:
                # ç”Ÿæˆé™éŸ³ç‰‡æ®µ
                silence_duration = note['duration'] * beat_duration
                silence_samples = int(silence_duration * sr)
                audio_segments.append((note['start_time'], np.zeros(silence_samples, dtype=np.float32)))
                continue
            
            # å¤„ç†æœªåŒ¹é…çš„æ™®é€šéŸ³ç¬¦ï¼ˆç”Ÿæˆé™éŸ³ï¼‰
            if not note['matched']:
                silence_duration = note['duration'] * beat_duration
                silence_samples = int(silence_duration * sr)
                audio_segments.append((note['start_time'], np.zeros(silence_samples, dtype=np.float32)))
                continue
            
            # å¤„ç†å·²åŒ¹é…çš„æ™®é€šéŸ³ç¬¦
            clip_id = note['clip_id']
            semitones = note['pitch_shift']
            
            # å¦‚æœå·²å¤„ç†è¿‡ç›¸åŒå˜è°ƒçš„ç‰‡æ®µï¼Œç›´æ¥é‡ç”¨
            cache_key = f"{clip_id}_{semitones}"
            if cache_key not in processed_clips:
                # åŠ è½½åŸå§‹éŸ³é¢‘
                clip = clip_manager.clips[clip_id]
                y, clip_sr = sf.read(clip['filepath'])
                if y.ndim > 1:
                    y = np.mean(y, axis=1)
                
                # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
                if clip_sr != sr:
                    y = librosa.resample(y, orig_sr=clip_sr, target_sr=sr)
                
                # å˜è°ƒå¤„ç†
                if use_pitch_shift and abs(semitones) > 0.1:
                    y = pitch_shift(y, sr, semitones)
                
                processed_clips[cache_key] = y
            
            # è·å–å¤„ç†åçš„éŸ³é¢‘
            y_processed = processed_clips[cache_key].copy()
            
            # æ—¶é—´æ‹‰ä¼¸ä»¥åŒ¹é…éŸ³ç¬¦æ—¶é•¿
            target_duration = note['duration'] * beat_duration
            current_duration = len(y_processed) / sr
            
            if abs(current_duration - target_duration) > 0.01:  # 10mså®¹å·®
                rate = current_duration / target_duration
                rate = np.clip(rate, 0.5, 2.0)  # é™åˆ¶æ‹‰ä¼¸èŒƒå›´
                y_processed = librosa.effects.time_stretch(y_processed, rate=rate)
            
            # è°ƒæ•´åˆ°ç²¾ç¡®é•¿åº¦
            target_samples = int(target_duration * sr)
            if len(y_processed) > target_samples:
                y_processed = y_processed[:target_samples]
            else:
                y_processed = np.pad(y_processed, (0, target_samples - len(y_processed)), mode='constant')
            
            # åº”ç”¨éŸ³é‡è°ƒæ•´ï¼ˆåŸºäºvelocityï¼‰
            velocity_factor = note['velocity'] / 127.0
            y_processed *= velocity_factor * 0.7  # é¿å…è¿‡è½½
            
            # æ·»åŠ æ·¡å…¥æ·¡å‡º
            y_processed = apply_fade(y_processed, sr, fade_in=0.02, fade_out=0.05)
            
            audio_segments.append((note['start_time'], y_processed))
            
            # æ¯å¤„ç†10ä¸ªç‰‡æ®µæ›´æ–°ä¸€æ¬¡çŠ¶æ€
            if i % 10 == 0 and i > 0:
                processed_count = len([n for n in notes[:i+1] if not n.get('is_rest') and n['midi_pitch'] != -1])
                generation_status = f"âœ… å·²å¤„ç† {processed_count}/{total_valid_notes} ä¸ªéŸ³ç¬¦\nğŸ”„ ç»§ç»­å¤„ç†éŸ³é¢‘..."
                yield None, generation_status, match_details, "å¤„ç†ä¸­..."
        
        generation_status = f"âœ… éŸ³é¢‘å¤„ç†å®Œæˆï¼Œå…± {len(audio_segments)} ä¸ªéŸ³é¢‘ç‰‡æ®µ\nğŸ”„ å¼€å§‹æ‹¼æ¥éŸ³ä¹..."
        yield None, generation_status, match_details, "æ‹¼æ¥ä¸­..."
        
        # 4. æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ - å…³é”®ä¿®å¤éƒ¨åˆ†
        # è®¡ç®—æ€»æ—¶é•¿ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰
        max_end_time_seconds = 0
        generation_status = f"ğŸ”„ æ­£åœ¨è®¡ç®—æ€»æ—¶é•¿..."
        yield None, generation_status, match_details, "è®¡ç®—æ—¶é•¿ä¸­..."
        
        for start_time, segment in audio_segments:
            segment_duration = len(segment) / sr
            end_time_seconds = start_time * beat_duration + segment_duration
            if end_time_seconds > max_end_time_seconds:
                max_end_time_seconds = end_time_seconds
        
        generation_status = f"âœ… æ€»æ—¶é•¿è®¡ç®—å®Œæˆ: {max_end_time_seconds:.2f}ç§’\nğŸ”„ æ­£åœ¨åˆ†é…å†…å­˜..."
        yield None, generation_status, match_details, "åˆ†é…å†…å­˜ä¸­..."
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç©ºé—´ï¼ŒåŠ ä¸Š0.5ç§’çš„ä½™é‡
        total_samples = int(max_end_time_seconds * sr) + int(0.5 * sr)
        final_audio = np.zeros(total_samples, dtype=np.float32)
        
        generation_status = f"âœ… å†…å­˜åˆ†é…å®Œæˆ: {total_samples}ä¸ªæ ·æœ¬\nğŸ”„ å¼€å§‹æ”¾ç½®éŸ³é¢‘ç‰‡æ®µ..."
        yield None, generation_status, match_details, "æ”¾ç½®ç‰‡æ®µä¸­..."
        
        # æŒ‰æ—¶é—´çº¿æ”¾ç½®éŸ³é¢‘ç‰‡æ®µ
        placed_count = 0
        for i, (start_time, segment) in enumerate(audio_segments):
            start_sample = int(start_time * beat_duration * sr)
            end_sample = start_sample + len(segment)
            
            # ç¡®ä¿ç‰‡æ®µåœ¨èŒƒå›´å†…
            if start_sample < len(final_audio):
                # è®¡ç®—å®é™…ç»“æŸä½ç½®
                end_actual = min(end_sample, len(final_audio))
                # ç¡®ä¿æ®µé•¿åº¦æ­£ç¡®
                segment_len = end_actual - start_sample
                if segment_len > 0:
                    # ä½¿ç”¨å åŠ è€Œä¸æ˜¯è¦†ç›–
                    final_audio[start_sample:end_actual] += segment[:segment_len]
                    placed_count += 1
            
            # æ¯æ”¾ç½®10ä¸ªç‰‡æ®µæ›´æ–°ä¸€æ¬¡çŠ¶æ€
            if i % 10 == 0 and i > 0:
                generation_status = f"ğŸ”„ å·²æ”¾ç½® {i+1}/{len(audio_segments)} ä¸ªç‰‡æ®µ..."
                yield None, generation_status, match_details, "æ”¾ç½®ç‰‡æ®µä¸­..."
        
        generation_status = f"âœ… ç‰‡æ®µæ”¾ç½®å®Œæˆ: {placed_count}/{len(audio_segments)} ä¸ªç‰‡æ®µ\nğŸ”„ æ­£åœ¨å½’ä¸€åŒ–..."
        yield None, generation_status, match_details, "å½’ä¸€åŒ–ä¸­..."
        
        # å½’ä¸€åŒ–
        final_audio = normalize_audio(final_audio)
        
        # æ·»åŠ æ·¡å‡ºæ•ˆæœï¼Œé¿å…çªç„¶ç»“æŸ
        fade_out_samples = int(0.05 * sr)
        if fade_out_samples > 0 and fade_out_samples <= len(final_audio):
            fade_out_window = np.linspace(1, 0, fade_out_samples)
            final_audio[-fade_out_samples:] *= fade_out_window
        
        generation_status = f"âœ… éŸ³é¢‘å¤„ç†å®Œæˆ\nğŸ”„ æ­£åœ¨ç”ŸæˆæŠ¥å‘Š..."
        yield None, generation_status, match_details, "ç”ŸæˆæŠ¥å‘Šä¸­..."
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        report = f"""
        ## ğŸµ éŸ³ä¹ç”ŸæˆæŠ¥å‘Š
        
        ### åŸºæœ¬ä¿¡æ¯
        - **ä¹è°±æ–‡ä»¶**: {os.path.basename(score_file)}
        - **éŸ³ç¬¦æ€»æ•°**: {len(notes)} (å«ä¼‘æ­¢ç¬¦)
        - **å¯åŒ¹é…éŸ³ç¬¦**: {total_valid_notes} (ä¸å«ä¼‘æ­¢ç¬¦)
        - **æ¼”å¥é€Ÿåº¦**: {tempo} BPM
        - **æ€»æ—¶é•¿**: {total_samples/sr:.2f} ç§’
        - **é‡‡æ ·ç‡**: {sr} Hz
        
        ### åŒ¹é…æƒ…å†µ
        - **æˆåŠŸåŒ¹é…**: {matched_count} ä¸ªå¯åŒ¹é…éŸ³ç¬¦ ({match_rate:.1f}%)
        - **éœ€è¦å˜è°ƒ**: {sum(1 for n in valid_notes if n['matched'] and abs(n.get('pitch_shift', 0)) > 0.1)} ä¸ª
        - **æœªåŒ¹é…**: {total_valid_notes - matched_count} ä¸ª
        - **ä¼‘æ­¢ç¬¦**: {len(notes) - total_valid_notes} ä¸ª
        
        ### éŸ³é¢‘å¤„ç†
        - **ç”Ÿæˆçš„ç‰‡æ®µ**: {len(audio_segments)} ä¸ª
        - **æˆåŠŸæ”¾ç½®**: {placed_count} ä¸ªç‰‡æ®µ
        - **å³°å€¼ç”µå¹³**: {np.max(np.abs(final_audio)):.3f}
        
        ### ä½¿ç”¨ç‰‡æ®µ
        """
        
        # ç»Ÿè®¡ä½¿ç”¨çš„ç‰‡æ®µ
        used_clips = {}
        for note in valid_notes:
            if note['matched']:
                clip_id = note['clip_id']
                used_clips[clip_id] = used_clips.get(clip_id, 0) + 1
        
        for clip_id, count in used_clips.items():
            clip = clip_manager.clips[clip_id]
            note_name = clip.get('note_info', {}).get('note', 'æœªçŸ¥')
            report += f"- **ç‰‡æ®µ{clip_id}** ({note_name}): ä½¿ç”¨ {count} æ¬¡\n"
        
        # >>> ä¿®æ”¹ç‚¹3ï¼šæ·»åŠ éŸ³è½¨ä¸ä¹å™¨ç»Ÿè®¡ <<<
        report += f"\n### éŸ³è½¨ä¸ä¹å™¨ä¿¡æ¯\n"
        # ç»Ÿè®¡éŸ³è½¨
        tracks_used = set(n.get('track', 0) for n in notes if n.get('track') is not None)
        report += f"- **ä½¿ç”¨éŸ³è½¨æ•°**: {len(tracks_used)} ä¸ª\n"
        
        # æŒ‰éŸ³è½¨ç»Ÿè®¡éŸ³ç¬¦
        if len(tracks_used) > 1:
            report += f"- **å„éŸ³è½¨éŸ³ç¬¦åˆ†å¸ƒ**:\n"
            for track_num in sorted(tracks_used):
                track_notes = [n for n in notes if n.get('track', 0) == track_num and n['midi_pitch'] != -1]
                if track_notes:
                    instr = track_notes[0].get('instrument', 'unknown')
                    report += f"  - éŸ³è½¨{track_num} ({instr}): {len(track_notes)} ä¸ªéŸ³ç¬¦\n"
        
        # ç»Ÿè®¡ä¹å™¨ï¼ˆä»…ç»Ÿè®¡éä¼‘æ­¢ç¬¦ï¼‰
        instruments_used = {}
        for note in valid_notes:
            instr = note.get('instrument', 'unknown')
            instruments_used[instr] = instruments_used.get(instr, 0) + 1
        
        if instruments_used:
            report += f"- **ä¹å™¨åˆ†å¸ƒ**:\n"
            for instr, count in sorted(instruments_used.items(), key=lambda x: x[1], reverse=True):
                report += f"  - {instr}: {count} ä¸ªéŸ³ç¬¦\n"
        
        report += f"\n### è°ƒè¯•ä¿¡æ¯\n"
        report += f"- **æœ€å¤§ç»“æŸæ—¶é—´**: {max_end_time_seconds:.2f} ç§’\n"
        report += f"- **æ€»æ ·æœ¬æ•°**: {total_samples} ä¸ª\n"
        report += f"- **å®é™…æ—¶é•¿**: {len(final_audio)/sr:.2f} ç§’\n"
        
        # å¦‚æœæœ‰åŸå§‹MIDIé€Ÿåº¦ä¿¡æ¯ï¼Œæ˜¾ç¤ºå¯¹æ¯”
        tempos = set(n.get('tempo') for n in notes if n.get('tempo'))
        if len(tempos) == 1:
            original_tempo = list(tempos)[0]
            report += f"- **åŸå§‹ä¹è°±é€Ÿåº¦**: {original_tempo:.0f} BPM\n"
            report += f"- **å®é™…ä½¿ç”¨é€Ÿåº¦**: {tempo} BPM\n"
        
        report += f"\nâ±ï¸ **ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}"
        
        # ä¿å­˜ç”Ÿæˆçš„éŸ³ä¹
        output_filename = f"auto_composition_{time.strftime('%Y%m%d_%H%M%S')}.wav"
        output_path = os.path.join(config.output_dir, output_filename)
        sf.write(output_path, final_audio, sr)
        
        generation_status = f"âœ… éŸ³ä¹ç”Ÿæˆå®Œæˆï¼\nğŸ“ å·²ä¿å­˜è‡³: {output_filename}"
        
        yield (sr, final_audio), report, match_details, generation_status
        
    except Exception as e:
        error_msg = f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
        print(f"ç”ŸæˆéŸ³ä¹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        yield None, error_msg, [], "ç”Ÿæˆå¤±è´¥"

# ========= åˆ›å»ºGradioç•Œé¢ =========

def build_advanced_ui():
    with gr.Blocks(title="é«˜çº§éŸ³é¢‘å¤„ç†ä¸éŸ³ä¹åˆ¶ä½œç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸµ é«˜çº§éŸ³é¢‘å¤„ç†ä¸éŸ³ä¹åˆ¶ä½œç³»ç»Ÿ
        
        ## åŠŸèƒ½ä»‹ç»
        1. **éŸ³é¢‘è¯†åˆ«**ï¼šè‡ªåŠ¨æ£€æµ‹éŸ³é¢‘é¢‘ç‡å¹¶è½¬æ¢ä¸ºéŸ³å
        2. **éŸ³é¢‘å¤„ç†**ï¼šæ”¯æŒå˜é€Ÿã€å˜è°ƒã€æ·¡å…¥æ·¡å‡º
        3. **éŸ³ä¹åˆ¶ä½œ**ï¼šæ ¹æ®ä¹è°±æˆ–æ‰‹åŠ¨ç¼–æ’åˆ¶ä½œéŸ³ä¹
        4. **é¢‘è°±åˆ†æ**ï¼šå¯è§†åŒ–éŸ³é¢‘ç‰¹å¾
        """)
        
        with gr.Tabs():
            with gr.TabItem("ğŸ¤ éŸ³é¢‘ä¸Šä¼ ä¸è¯†åˆ«"):
                with gr.Row():
                    with gr.Column(scale=1):
                        audio_input = gr.Audio(
                            label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶",
                            type="filepath"
                        )
                        target_note = gr.Textbox(
                            label="ç›®æ ‡éŸ³é«˜ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šC4, D#4, Gb5",
                            value=""
                        )
                        
                        with gr.Row():
                            auto_detect = gr.Checkbox(
                                label="è‡ªåŠ¨æ£€æµ‹éŸ³é«˜",
                                value=True
                            )
                            analysis_mode = gr.Radio(
                                choices=["simple", "enhanced"],
                                label="åˆ†ææ¨¡å¼",
                                value="simple",
                                info="ç®€å•æ¨¡å¼ï¼šé¢‘è°±å›¾+æ³¢å½¢å›¾ | å¢å¼ºæ¨¡å¼ï¼šå¤šç§åˆ†æå›¾è¡¨"
                            )
                        
                        btn_analyze = gr.Button("åˆ†æéŸ³é¢‘", variant="primary")
                        
                    with gr.Column(scale=2):
                        result_text = gr.Markdown(label="åˆ†æç»“æœ")
                        clip_id_output = gr.Number(
                            label="ç‰‡æ®µID",
                            visible=False
                        )
                        spectrogram = gr.Plot(
                            label="é¢‘è°±å›¾åˆ†æ"
                        )
                        enhanced_analysis = gr.Plot(
                            label="å¢å¼ºåˆ†æ",
                            visible=False
                        )
                        
                        def toggle_analysis(analysis_mode):
                            if analysis_mode == "enhanced":
                                return gr.Plot(visible=True)
                            else:
                                return gr.Plot(visible=False)
                        
                        analysis_mode.change(
                            fn=toggle_analysis,
                            inputs=[analysis_mode],
                            outputs=[enhanced_analysis]
                        )
                
                btn_analyze.click(
                    fn=handle_audio_upload,
                    inputs=[audio_input, target_note, auto_detect, analysis_mode],
                    outputs=[result_text, clip_id_output, spectrogram, enhanced_analysis]
                )
                
                # é¢‘è°±å›¾è¯´æ˜
                with gr.Accordion("ğŸ“Š é¢‘è°±å›¾è§£è¯»æŒ‡å—", open=False):
                    gr.Markdown("""
                    ### å¦‚ä½•è¯»æ‡‚é¢‘è°±å›¾ï¼š
                    
                    1. **æ—¶é—´è½´ï¼ˆXè½´ï¼‰**ï¼šä»å·¦åˆ°å³è¡¨ç¤ºéŸ³é¢‘çš„æ—¶é—´è¿›åº¦
                    2. **é¢‘ç‡è½´ï¼ˆYè½´ï¼‰**ï¼šä»ä¸‹åˆ°ä¸Šè¡¨ç¤ºå£°éŸ³é¢‘ç‡ï¼ˆä½éŸ³åœ¨ä¸‹ï¼Œé«˜éŸ³åœ¨ä¸Šï¼‰
                    3. **é¢œè‰²æ·±æµ…**ï¼šè¡¨ç¤ºéŸ³é‡å¤§å°
                       - **æ·±è‰²/è“è‰²**ï¼šå®‰é™çš„å£°éŸ³
                       - **äº®è‰²/é»„è‰²**ï¼šå“äº®çš„å£°éŸ³
                    4. **çº¢è‰²è™šçº¿**ï¼šæ£€æµ‹åˆ°çš„ä¸»éŸ³é«˜é¢‘ç‡
                    5. **åº•éƒ¨æ³¢å½¢å›¾**ï¼šéŸ³é¢‘çš„æŒ¯å¹…å˜åŒ–
                    
                    ### å¸¸è§éŸ³é¢‘åœ¨é¢‘è°±å›¾ä¸Šçš„è¡¨ç°ï¼š
                    - **çº¯éŸ³/ä¹å™¨å•éŸ³**ï¼šä¸€æ¡æ¸…æ™°çš„æ°´å¹³çº¿
                    - **äººå£°/å¤æ‚éŸ³è‰²**ï¼šå¤šæ¡æ°´å¹³çº¿ï¼ˆåŸºé¢‘+æ³›éŸ³ï¼‰
                    - **å™ªéŸ³/æ‰“å‡»ä¹**ï¼šå‚ç›´çš„è‰²å—ï¼ˆçŸ­æš‚çˆ†å‘ï¼‰
                    - **é™éŸ³**ï¼šæ·±è‰²æˆ–é»‘è‰²åŒºåŸŸ
                    
                    ### å¦‚ä½•åˆ¤æ–­éŸ³é«˜ï¼š
                    - å¯»æ‰¾æœ€äº®çš„æ°´å¹³çº¿æ¡
                    - å¯¹ç…§å³ä¾§é¢‘ç‡æ ‡å°º
                    - çº¢è‰²è™šçº¿æ ‡è®°çš„æ˜¯ç³»ç»Ÿæ£€æµ‹åˆ°çš„ä¸»é¢‘ç‡
                    """)
            
            with gr.TabItem("ğŸ›ï¸ éŸ³é¢‘å¤„ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        clip_id_input = gr.Number(
                            label="ç‰‡æ®µID",
                            value=0,
                            precision=0
                        )
                        operation = gr.Radio(
                            choices=["time_stretch", "pitch_shift"],
                            label="å¤„ç†ç±»å‹",
                            value="time_stretch"
                        )
                        value_input = gr.Slider(
                            label="å‚æ•°å€¼",
                            minimum=0.1,
                            maximum=5.0,
                            value=1.0,
                            step=0.1,
                            visible=True
                        )
                        
                        def update_slider(operation):
                            if operation == "time_stretch":
                                return gr.Slider(
                                    minimum=0.1,
                                    maximum=5.0,
                                    value=1.0,
                                    step=0.1,
                                    label="ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰"
                                )
                            else:
                                return gr.Slider(
                                    minimum=-12,
                                    maximum=12,
                                    value=0,
                                    step=0.5,
                                    label="åŠéŸ³ç§»åŠ¨"
                                )
                        
                        operation.change(
                            fn=update_slider,
                            inputs=[operation],
                            outputs=[value_input]
                        )
                        
                        btn_process = gr.Button("å¤„ç†éŸ³é¢‘", variant="primary")
                        process_result = gr.Markdown(label="å¤„ç†ç»“æœ")
                    
                    with gr.Column(scale=2):
                        audio_preview = gr.Audio(
                            label="å¤„ç†ç»“æœé¢„è§ˆ",
                            type="numpy"
                        )
                
                btn_process.click(
                    fn=process_audio_clip,
                    inputs=[clip_id_input, operation, value_input],
                    outputs=[process_result, audio_preview]
                )
            
            with gr.TabItem("ğŸ¹ éŸ³ä¹åˆ¶ä½œ"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("""
                        ### éŸ³ä¹ç¼–æ’è¯´æ˜
                        
                        æ ¼å¼ï¼š`æ‹æ•°:ç‰‡æ®µID, æ‹æ•°:ç‰‡æ®µID, ...`
                        
                        ç¤ºä¾‹ï¼š
                        ```
                        0:0, 1:1, 2:2, 4:3
                        ```
                        
                        è¡¨ç¤ºï¼š
                        - ç¬¬0æ‹ä½¿ç”¨ç‰‡æ®µ0
                        - ç¬¬1æ‹ä½¿ç”¨ç‰‡æ®µ1  
                        - ç¬¬2æ‹ä½¿ç”¨ç‰‡æ®µ2
                        - ç¬¬4æ‹ä½¿ç”¨ç‰‡æ®µ3
                        """)
                        
                        clip_assignments = gr.Textbox(
                            label="ç‰‡æ®µåˆ†é…",
                            placeholder="æ ¼å¼: æ‹æ•°:ç‰‡æ®µID, æ‹æ•°:ç‰‡æ®µID,...",
                            lines=3
                        )
                        tempo_input = gr.Slider(
                            label="é€Ÿåº¦ (BPM)",
                            minimum=40,
                            maximum=240,
                            value=120,
                            step=10
                        )
                        btn_compose = gr.Button("ç”ŸæˆéŸ³ä¹", variant="primary")
                        compose_result = gr.Markdown(label="ç”Ÿæˆç»“æœ")
                    
                    with gr.Column(scale=2):
                        composition_audio = gr.Audio(
                            label="ç”ŸæˆéŸ³ä¹",
                            type="numpy"
                        )
                
                btn_compose.click(
                    fn=generate_music_from_clips,
                    inputs=[clip_assignments, tempo_input],
                    outputs=[compose_result, composition_audio]
                )
            
            with gr.TabItem("ğŸ“‹ ç‰‡æ®µç®¡ç†"):
                def update_clips_table():
                    clips = clip_manager.get_all_clips()
                    table_data = []
                    for clip in clips:
                        note_info = clip.get('note_info', {})
                        table_data.append([
                            clip['id'],
                            clip['filename'],
                            note_info.get('note', 'æœªçŸ¥'),
                            f"{note_info.get('frequency', 0):.1f}" if note_info.get('frequency') else 'æœªçŸ¥',
                            f"{note_info.get('cents', 0):+.1f}" if note_info.get('cents') is not None else '',
                            f"{clip['duration']:.2f}",
                            clip['created_at']
                        ])
                    return table_data
                
                with gr.Row():
                    clips_table = gr.Dataframe(
                        headers=["ID", "æ–‡ä»¶å", "éŸ³å", "é¢‘ç‡", "åå·®", "æ—¶é•¿", "åˆ›å»ºæ—¶é—´"],
                        label="æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ",
                        datatype=["number", "str", "str", "str", "str", "str", "str"],
                        row_count=10,
                        col_count=7,
                        interactive=False
                    )
                
                with gr.Row():
                    btn_refresh = gr.Button("åˆ·æ–°åˆ—è¡¨")
                    delete_clip_id = gr.Number(
                        label="åˆ é™¤ç‰‡æ®µID",
                        value=0,
                        precision=0
                    )
                    btn_delete = gr.Button("åˆ é™¤ç‰‡æ®µ", variant="stop")
                
                with gr.Row():
                    btn_cleanup = gr.Button("æ¸…ç†å­¤ç«‹æ–‡ä»¶", variant="secondary")

                def cleanup_orphaned():
                    clip_manager.cleanup_orphaned_files()
                    return "å·²æ¸…ç†å­¤ç«‹æ–‡ä»¶", update_clips_table()

                btn_cleanup.click(
                    fn=cleanup_orphaned,
                    inputs=[],
                    outputs=[compose_result, clips_table]
                )
                
                def delete_selected_clip(clip_id):
                    success = clip_manager.delete_clip(int(clip_id))
                    if success:
                        return f"âœ… å·²åˆ é™¤ç‰‡æ®µ {clip_id}", update_clips_table()
                    else:
                        return f"âŒ åˆ é™¤å¤±è´¥ï¼Œç‰‡æ®µ {clip_id} ä¸å­˜åœ¨", update_clips_table()
                
                btn_refresh.click(
                    fn=update_clips_table,
                    inputs=[],
                    outputs=[clips_table]
                )
                
                btn_delete.click(
                    fn=delete_selected_clip,
                    inputs=[delete_clip_id],
                    outputs=[compose_result, clips_table]
                )

            build_music_composition_tab()
        gr.Markdown("""
        ## ğŸ“š ä½¿ç”¨è¯´æ˜
        
        ### 1. éŸ³é¢‘è¯†åˆ«
        - ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒwav, mp3ç­‰æ ¼å¼ï¼‰
        - ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹éŸ³é«˜å¹¶æ˜¾ç¤ºé¢‘è°±å›¾
        - å¯è¾“å…¥ç›®æ ‡éŸ³é«˜è¿›è¡Œæ¯”è¾ƒ
        
        ### 2. éŸ³é¢‘å¤„ç†
        - **æ—¶é—´æ‹‰ä¼¸**ï¼šè°ƒæ•´éŸ³é¢‘æ—¶é•¿è€Œä¸æ”¹å˜éŸ³é«˜
        - **éŸ³é«˜ç§»åŠ¨**ï¼šè°ƒæ•´éŸ³é«˜è€Œä¸æ”¹å˜æ—¶é•¿
        - å¤„ç†åçš„éŸ³é¢‘ä¼šä¿å­˜ä¸ºæ–°ç‰‡æ®µ
        
        ### 3. éŸ³ä¹åˆ¶ä½œ
        - å°†éŸ³é¢‘ç‰‡æ®µåˆ†é…åˆ°ç‰¹å®šçš„æ‹æ•°ä½ç½®
        - è°ƒæ•´éŸ³ä¹é€Ÿåº¦ï¼ˆBPMï¼‰
        - ç³»ç»Ÿä¼šè‡ªåŠ¨æ‹¼æ¥ç”Ÿæˆå®Œæ•´éŸ³ä¹
        
        ### 4. é¢‘è°±å›¾è§£è¯»
        - **æ°´å¹³çº¿**ï¼šç¨³å®šçš„éŸ³é«˜
        - **å‚ç›´çº¿**ï¼šç¬æ—¶å£°éŸ³ï¼ˆå¦‚é¼“ç‚¹ï¼‰
        - **é¢œè‰²æ·±æµ…**ï¼šéŸ³é‡å¤§å°
        - **çº¢è‰²è™šçº¿**ï¼šæ£€æµ‹åˆ°çš„ä¸»é¢‘ç‡
        
        ## âš™ï¸ å®‰è£…è¯´æ˜
        ```bash
        # åŸºæœ¬ä¾èµ–
        pip install gradio librosa numpy soundfile matplotlib scipy
        
        # è§£å†³ä¸­æ–‡å­—ä½“é—®é¢˜ï¼ˆWindowsï¼‰
        # ç¡®ä¿ç³»ç»Ÿå·²å®‰è£…ä¸­æ–‡å­—ä½“ï¼ˆå¦‚å¾®è½¯é›…é»‘ï¼‰
        
        # è§£å†³ä¸­æ–‡å­—ä½“é—®é¢˜ï¼ˆLinuxï¼‰
        sudo apt-get install fonts-wqy-microhei
        ```
        """)
    
    return demo

# ========= ä¸»ç¨‹åº =========

if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶
    config_data = {
        "audio_settings": {
            "sample_rate": 22050,
            "min_freq": 32.70,
            "max_freq": 4186.01,
            "tempo": 120
        },
        "detection_settings": {
            "silence_threshold_db": 40,
            "confidence_threshold": 0.7,
            "min_clip_duration": 0.05
        },
        "processing_settings": {
            "time_stretch_range": [0.5, 2.0],
            "pitch_shift_range": [-12, 12],
            "fade_in": 0.01,
            "fade_out": 0.01
        }
    }
    
    with open('config.json', 'w', encoding='utf-8') as f:
        json.dump(config_data, f, ensure_ascii=False, indent=2)
    
    # å¯åŠ¨åº”ç”¨
    print("=" * 60)
    print("å¯åŠ¨é«˜çº§éŸ³é¢‘å¤„ç†ä¸éŸ³ä¹åˆ¶ä½œç³»ç»Ÿ")
    print("è¯·è®¿é—® http://localhost:7860 æ‰“å¼€ç•Œé¢")
    print("=" * 60)
    
    app = build_advanced_ui()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )