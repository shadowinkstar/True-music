import os
import time
from typing import List
import re

import librosa
import numpy as np
import soundfile as sf

from .audio_processing import apply_fade, normalize_audio, pitch_shift
from .context import get_config, require_clip_manager
from .matching import find_best_match_for_note
from .score_parser import parse_score_notes


def generate_music_from_clips(clip_assignments, tempo):
    """ä»ç‰‡æ®µç”ŸæˆéŸ³ä¹"""
    config = get_config()
    clip_manager = require_clip_manager()

    sr = config.sample_rate
    beat_duration = 60.0 / tempo

    # è§£æç‰‡æ®µåˆ†é…ï¼ˆæ ¼å¼: "æ—¶é—´æ‹:ç‰‡æ®µID,æ—¶é—´æ‹:ç‰‡æ®µID,..."ï¼‰
    assignments = []
    for assignment in clip_assignments.split(","):
        if ":" in assignment:
            beat_str, clip_id_str = assignment.split(":")
            try:
                beat = float(beat_str.strip())
                clip_id = int(clip_id_str.strip())
                assignments.append((beat, clip_id))
            except Exception:
                continue

    if not assignments:
        return "æ²¡æœ‰æœ‰æ•ˆçš„ç‰‡æ®µåˆ†é…", None

    # æŒ‰æ—¶é—´æ’åº
    assignments.sort(key=lambda x: x[0])

    # è®¡ç®—æ€»æ—¶é•¿
    last_beat = max([a[0] for a in assignments]) + 4  # å‡è®¾æ¯ä¸ªç‰‡æ®µ4æ‹
    total_samples = int(last_beat * beat_duration * sr)

    # åˆ›å»ºéŸ³è½¨
    track = np.zeros(total_samples)

    for beat, clip_id in assignments:
        if 0 <= clip_id < len(clip_manager.clips):
            clip = clip_manager.clips[clip_id]
            y, _ = sf.read(clip["filepath"])
            if y.ndim > 1:
                y = np.mean(y, axis=1)

            # è°ƒæ•´åˆ°æ ‡å‡†æ—¶é•¿ï¼ˆ1æ‹ï¼‰
            target_samples = int(beat_duration * sr)
            if len(y) > target_samples:
                y = y[:target_samples]
            else:
                y = np.pad(y, (0, target_samples - len(y)), mode="constant")

            # æ·»åŠ åˆ°éŸ³è½¨
            start_sample = int(beat * beat_duration * sr)
            end_sample = start_sample + len(y)

            if end_sample <= len(track):
                track[start_sample:end_sample] += y

    # å½’ä¸€åŒ–
    track = normalize_audio(track)

    # ä¿å­˜ç»“æœ
    output_filename = f"composition_{time.strftime('%Y%m%d_%H%M%S')}.wav"
    output_path = os.path.join(config.output_dir, output_filename)
    sf.write(output_path, track, sr)

    return f"âœ… éŸ³ä¹ç”Ÿæˆå®Œæˆ: {output_filename}", (sr, track)


def _parse_source_sequence(sequence_text: str) -> List[str]:
    if not sequence_text:
        return []
    tokens = re.split(r"[,\sï¼Œ;ï¼›]+", sequence_text.strip())
    return [token for token in tokens if token]


def auto_generate_music_from_score(
    score_file,
    tempo=120,
    tolerance_cents=20.0,
    use_pitch_shift=True,
    source_sequence_text="",
):
    """
    è‡ªåŠ¨ä»ä¹è°±ç”ŸæˆéŸ³ä¹çš„ä¸»å‡½æ•°
    """
    config = get_config()
    clip_manager = require_clip_manager()

    if not score_file:
        return None, "è¯·å…ˆä¸Šä¼ ä¹è°±æ–‡ä»¶", [], "âš ï¸ æœªä¸Šä¼ ä¹è°±"

    try:
        generation_status = "ğŸš€ å¼€å§‹è§£æä¹è°±..."
        yield None, generation_status, [], "è§£æä¸­..."

        # 1. è§£æä¹è°±
        notes = parse_score_notes(score_file)
        if not notes:
            return None, "âŒ æœªèƒ½ä»ä¹è°±ä¸­è§£æå‡ºéŸ³ç¬¦", [], "è§£æå¤±è´¥"

        # ============ æ–°å¢ï¼šä¼˜å…ˆä½¿ç”¨MIDIæ–‡ä»¶çš„åŸå§‹é€Ÿåº¦ ============
        # æ£€æŸ¥éŸ³ç¬¦ä¸­æ˜¯å¦åŒ…å«åŸå§‹é€Ÿåº¦ä¿¡æ¯
        original_tempos = set(n.get("tempo") for n in notes if n.get("tempo") is not None)
        if original_tempos and len(original_tempos) == 1:
            original_tempo = list(original_tempos)[0]
            if original_tempo != tempo:
                print(
                    f"[INFO] ä½¿ç”¨MIDIæ–‡ä»¶åŸå§‹é€Ÿåº¦: {original_tempo} BPM (è¦†ç›–ç”¨æˆ·è®¾ç½®çš„ {tempo} BPM)"
                )
                tempo = original_tempo
        # ==========================================================

        generation_status = (
            f"âœ… è§£æå®Œæˆï¼Œå…± {len(notes)} ä¸ªéŸ³ç¬¦\nğŸ” å¼€å§‹åŒ¹é…éŸ³é¢‘ç‰‡æ®µ..."
        )
        yield None, generation_status, [], "åŒ¹é…ä¸­..."

        # 2. åŒ¹é…éŸ³é¢‘ç‰‡æ®µ
        sr = config.sample_rate
        beat_duration = 60.0 / tempo
        match_details = []
        source_sequence = _parse_source_sequence(source_sequence_text)
        sequence_index = 0

        # ============ æ–°å¢ï¼šè®¡ç®—ä¹è°±åŸå§‹ç†è®ºæ—¶é•¿ï¼ˆç”¨äºè°ƒè¯•ï¼‰ ============
        max_beat_in_score = max([note["start_time"] + note["duration"] for note in notes])
        theory_total_seconds = max_beat_in_score * beat_duration
        print(
            f"[DEBUG_TIMING] ä¹è°±ç†è®ºä¿¡æ¯: æ€»æ‹æ•°={max_beat_in_score:.2f}, tempo={tempo}, ç†è®ºæ—¶é•¿={theory_total_seconds:.2f}ç§’"
        )
        # ==========================================================

        # ä¸ºæ¯ä¸ªéŸ³ç¬¦åŒ¹é…ç‰‡æ®µ
        for i, note in enumerate(notes):
            target_midi = note["midi_pitch"]

            # >>> ä¿®æ”¹ç‚¹1ï¼šä¼˜å…ˆå¤„ç†ä¼‘æ­¢ç¬¦ <<<
            if target_midi == -1:
                note["matched"] = True
                note["is_rest"] = True
                match_details.append(
                    [
                        f"éŸ³ç¬¦{i+1}",
                        note["note_name"],
                        f"ä¼‘æ­¢ç¬¦ ({note['duration']:.2f}æ‹)",
                        "N/A",
                        "â¸ï¸ ä¼‘æ­¢",
                        note.get("track", 0),  # å±•ç¤ºéŸ³è½¨ä¿¡æ¯
                        note.get("instrument", "rest"),
                    ]
                )
                continue  # è·³è¿‡åç»­åŒ¹é…é€»è¾‘

            required_tag = (
                source_sequence[sequence_index % len(source_sequence)]
                if source_sequence
                else None
            )

            # å¯»æ‰¾æœ€ä½³åŒ¹é…ï¼ˆä»…é’ˆå¯¹æ™®é€šéŸ³ç¬¦ï¼‰
            best_clip, semitones_diff = find_best_match_for_note(
                target_midi,
                tolerance_cents,
                required_tag=required_tag,
            )

            if best_clip:
                note["matched"] = True
                note["clip_id"] = best_clip["id"]
                note["pitch_shift"] = semitones_diff if use_pitch_shift else 0

                match_status = (
                    "âœ… å®Œå…¨åŒ¹é…"
                    if abs(semitones_diff) < 0.1
                    else f"ğŸ›ï¸ éœ€å˜è°ƒ {semitones_diff:+.1f} åŠéŸ³"
                )

                match_details.append(
                    [
                        f"éŸ³ç¬¦{i+1}",
                        note["note_name"],
                        f"ç‰‡æ®µ{best_clip['id']} ({best_clip.get('note_info', {}).get('note', 'æœªçŸ¥')})"
                        + (f" [{required_tag}]" if required_tag else ""),
                        f"{semitones_diff:+.1f}" if use_pitch_shift else "0",
                        match_status,
                        note.get("track", 0),  # æ–°å¢ï¼šå±•ç¤ºéŸ³è½¨ä¿¡æ¯
                        note.get("instrument", "unknown"),  # æ–°å¢ï¼šå±•ç¤ºä¹å™¨ä¿¡æ¯
                    ]
                )
            else:
                note["matched"] = False
                match_details.append(
                    [
                        f"éŸ³ç¬¦{i+1}",
                        note["note_name"],
                        f"æ— å¯ç”¨ç‰‡æ®µ{f' (æ ‡ç­¾: {required_tag})' if required_tag else ''}",
                        "N/A",
                        "âŒ æœªåŒ¹é…",
                        note.get("track", 0),
                        note.get("instrument", "unknown"),
                    ]
                )

            sequence_index += 1

        # ç»Ÿè®¡åŒ¹é…ç»“æœï¼ˆä»…ç»Ÿè®¡æ™®é€šéŸ³ç¬¦ï¼Œæ’é™¤ä¼‘æ­¢ç¬¦ï¼‰
        valid_notes = [n for n in notes if n.get("midi_pitch", 0) != -1]
        matched_count = sum(1 for n in valid_notes if n["matched"])
        total_valid_notes = len(valid_notes)
        match_rate = matched_count / total_valid_notes * 100 if total_valid_notes > 0 else 0

        generation_status = (
            f"âœ… åŒ¹é…å®Œæˆ: {matched_count}/{total_valid_notes} ä¸ªå¯åŒ¹é…éŸ³ç¬¦ ({match_rate:.1f}%)\nğŸ› ï¸ å¼€å§‹å¤„ç†éŸ³é¢‘..."
        )
        yield None, generation_status, match_details, "å¤„ç†ä¸­..."

        # 3. å¤„ç†éŸ³é¢‘ç‰‡æ®µ
        processed_clips = {}
        audio_segments = []

        for i, note in enumerate(notes):
            # ============ æ–°å¢ï¼šæ—¶é—´è°ƒè¯•ä¿¡æ¯ ============
            debug_info = (
                f"éŸ³ç¬¦{i}({note['note_name']}): start={note['start_time']:.2f}æ‹, dur={note['duration']:.2f}æ‹"
            )
            # ==========================================

            # >>> ä¿®æ”¹ç‚¹2ï¼šä¼˜å…ˆå¤„ç†ä¼‘æ­¢ç¬¦ <<<
            if note.get("is_rest") or note["midi_pitch"] == -1:
                # ç”Ÿæˆé™éŸ³ç‰‡æ®µ - å…³é”®ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—ç§’æ•°
                silence_duration = note["duration"] * beat_duration  # æ‹ â†’ ç§’
                silence_samples = int(silence_duration * sr)
                # å…³é”®ï¼šå­˜å‚¨å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè€Œä¸æ˜¯æ‹æ•°
                start_time_seconds = note["start_time"] * beat_duration
                audio_segments.append(
                    (start_time_seconds, np.zeros(silence_samples, dtype=np.float32))
                )

                print(
                    f"[DEBUG_TIMING] {debug_info} -> ä¼‘æ­¢ç¬¦: {silence_duration:.3f}ç§’, å¼€å§‹äº{start_time_seconds:.3f}ç§’"
                )
                continue

            # å¤„ç†æœªåŒ¹é…çš„æ™®é€šéŸ³ç¬¦ï¼ˆç”Ÿæˆé™éŸ³ï¼‰
            if not note["matched"]:
                silence_duration = note["duration"] * beat_duration  # æ‹ â†’ ç§’
                silence_samples = int(silence_duration * sr)
                # å…³é”®ï¼šå­˜å‚¨å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè€Œä¸æ˜¯æ‹æ•°
                start_time_seconds = note["start_time"] * beat_duration
                audio_segments.append(
                    (start_time_seconds, np.zeros(silence_samples, dtype=np.float32))
                )

                print(
                    f"[DEBUG_TIMING] {debug_info} -> æœªåŒ¹é…é™éŸ³: {silence_duration:.3f}ç§’, å¼€å§‹äº{start_time_seconds:.3f}ç§’"
                )
                continue

            # å¤„ç†å·²åŒ¹é…çš„æ™®é€šéŸ³ç¬¦
            clip_id = note["clip_id"]
            semitones = note["pitch_shift"]

            # å¦‚æœå·²å¤„ç†è¿‡ç›¸åŒå˜è°ƒçš„ç‰‡æ®µï¼Œç›´æ¥é‡ç”¨
            cache_key = f"{clip_id}_{semitones:.2f}"  # å›ºå®šå°æ•°ä½æ•°ï¼Œé¿å…æµ®ç‚¹è¯¯å·®
            if cache_key not in processed_clips:
                # åŠ è½½åŸå§‹éŸ³é¢‘
                clip = clip_manager.clips[clip_id]
                y, clip_sr = sf.read(clip["filepath"])
                if y.ndim > 1:
                    y = np.mean(y, axis=1)

                # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
                if clip_sr != sr:
                    y = librosa.resample(y, orig_sr=clip_sr, target_sr=sr)

                # å˜è°ƒå¤„ç†
                if use_pitch_shift and abs(semitones) > 0.1:
                    y = pitch_shift(y, sr, semitones)

                processed_clips[cache_key] = y

            # è·å–å¤„ç†åçš„éŸ³é¢‘
            y_processed = processed_clips[cache_key].copy()

            # æ—¶é—´æ‹‰ä¼¸ä»¥åŒ¹é…éŸ³ç¬¦æ—¶é•¿ - å…³é”®ï¼šç›®æ ‡æ—¶é•¿å·²ç»æ˜¯ç§’
            target_duration = note["duration"] * beat_duration
            current_duration = len(y_processed) / sr

            print(
                f"[DEBUG_TIMING] {debug_info} -> éŸ³é¢‘: å½“å‰{current_duration:.3f}ç§’, ç›®æ ‡{target_duration:.3f}ç§’"
            )

            # åªåœ¨å°èŒƒå›´å†…ä½¿ç”¨æ—¶é—´æ‹‰ä¼¸
            if 0.8 <= current_duration / target_duration <= 1.2:
                # å·®å¼‚åœ¨Â±20%ä»¥å†…ï¼Œä½¿ç”¨æ—¶é—´æ‹‰ä¼¸
                rate = current_duration / target_duration
                y_processed = librosa.effects.time_stretch(y_processed, rate=rate)
                print(f"[DEBUG_TIMING] å°èŒƒå›´æ‹‰ä¼¸: æ¯”ç‡{rate:.3f}")

            # å¼ºåˆ¶åŒ¹é…ç›®æ ‡é•¿åº¦ï¼ˆè£å‰ªæˆ–å¡«å……ï¼‰
            target_samples = int(target_duration * sr)
            if len(y_processed) != target_samples:
                # ä½¿ç”¨æ›´æ™ºèƒ½çš„è£å‰ª/å¡«å……
                if len(y_processed) > target_samples:
                    # ä»ä¸­é—´è£å‰ªï¼Œä¿æŒéŸ³ç¬¦ä¸»ä½“
                    start = (len(y_processed) - target_samples) // 2
                    y_processed = y_processed[start : start + target_samples]
                else:
                    # å¡«å……é™éŸ³
                    y_processed = np.pad(
                        y_processed,
                        (0, target_samples - len(y_processed)),
                        mode="constant",
                    )

            print(f"[DEBUG_TIMING] é•¿åº¦è°ƒæ•´: {len(y_processed)/sr:.3f}ç§’")

            # åº”ç”¨éŸ³é‡è°ƒæ•´ï¼ˆåŸºäºvelocityï¼‰ - ä¿æŒåŸMIDIå“åº¦å…³ç³»
            velocity_factor = note["velocity"] / 127.0  # æ ‡å‡†MIDIçº¿æ€§æ˜ å°„

            # ä½¿ç”¨çº¿æ€§æ˜ å°„ï¼Œä¿æŒä¸åŸMIDIä¸€è‡´çš„å“åº”
            # å»æ‰æ›²çº¿è°ƒæ•´å’Œå›ºå®šç³»æ•°ï¼Œè®©velocityç›´æ¥æ§åˆ¶å¢ç›Š
            y_processed *= velocity_factor

            # æ·»åŠ å³°å€¼é™åˆ¶ï¼ˆé˜²æ­¢å‰Šæ³¢ï¼Œä½†ä¿æŒç›¸å¯¹å¹³è¡¡ï¼‰
            max_amp = np.max(np.abs(y_processed))
            if max_amp > 1.0:  # åªåœ¨å®é™…å‰Šæ³¢æ—¶é™åˆ¶
                y_processed *= 0.99 / max_amp  # é™ä½åˆ°99%é¿å…å‰Šæ³¢
                print(f"[DEBUG] éŸ³ç¬¦{note.get('pitch', '?')}: é™åˆ¶å³°å€¼ {max_amp:.3f} -> 0.99")

            # å…ˆç¡®ä¿é•¿åº¦æ­£ç¡®ï¼Œå†æ·»åŠ æ·¡å…¥æ·¡å‡º
            target_samples = int(note["duration"] * beat_duration * sr)
            if len(y_processed) != target_samples:
                if len(y_processed) > target_samples:
                    # è£å‰ªä¸­é—´éƒ¨åˆ†ï¼Œä¿æŒéŸ³ç¬¦ä¸»ä½“
                    start = (len(y_processed) - target_samples) // 2
                    y_processed = y_processed[start : start + target_samples]
                else:
                    # å¡«å……é™éŸ³
                    y_processed = np.pad(
                        y_processed,
                        (0, target_samples - len(y_processed)),
                        mode="constant",
                    )

            # æ·»åŠ æ·¡å…¥æ·¡å‡ºï¼ˆé¿å…åº”ç”¨äºéå¸¸çŸ­çš„éŸ³ç¬¦ï¼‰
            min_length_for_fade = int(0.05 * sr)  # è‡³å°‘50ms
            if len(y_processed) > min_length_for_fade:
                # è‡ªé€‚åº”æ·¡å…¥æ·¡å‡ºï¼šçŸ­éŸ³ç¬¦ç”¨è¾ƒçŸ­æ·¡å‡ºï¼Œé•¿éŸ³ç¬¦ç”¨æ ‡å‡†æ·¡å‡º
                note_duration = len(y_processed) / sr

                if note_duration < 0.2:  # çŸ­éŸ³ç¬¦ (<200ms)
                    fade_in = min(0.01, note_duration * 0.1)
                    fade_out = min(0.02, note_duration * 0.2)
                else:  # æ­£å¸¸é•¿åº¦éŸ³ç¬¦
                    fade_in = 0.02
                    fade_out = 0.05

                y_processed = apply_fade(
                    y_processed, sr, fade_in=fade_in, fade_out=fade_out
                )

            # å…³é”®ï¼šå­˜å‚¨å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè€Œä¸æ˜¯æ‹æ•°
            start_time_seconds = note["start_time"] * beat_duration
            audio_segments.append((start_time_seconds, y_processed))

            # æ¯å¤„ç†10ä¸ªç‰‡æ®µæ›´æ–°ä¸€æ¬¡çŠ¶æ€
            if i % 10 == 0 and i > 0:
                processed_count = len(
                    [
                        n
                        for n in notes[: i + 1]
                        if not n.get("is_rest") and n["midi_pitch"] != -1
                    ]
                )
                generation_status = (
                    f"âœ… å·²å¤„ç† {processed_count}/{total_valid_notes} ä¸ªéŸ³ç¬¦\nğŸ› ï¸ ç»§ç»­å¤„ç†éŸ³é¢‘..."
                )
                yield None, generation_status, match_details, "å¤„ç†ä¸­..."

        generation_status = (
            f"âœ… éŸ³é¢‘å¤„ç†å®Œæˆï¼Œå…± {len(audio_segments)} ä¸ªéŸ³é¢‘ç‰‡æ®µ\nğŸ¼ å¼€å§‹æ‹¼æ¥éŸ³ä¹..."
        )
        yield None, generation_status, match_details, "æ‹¼æ¥ä¸­..."

        # 4. æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ - å…³é”®ä¿®å¤ï¼šæ‰€æœ‰æ—¶é—´éƒ½ä»¥ç§’ä¸ºå•ä½
        # è®¡ç®—æ€»æ—¶é•¿ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰
        max_end_time_seconds = 0
        generation_status = "â±ï¸ æ­£åœ¨è®¡ç®—æ€»æ—¶é•¿..."
        yield None, generation_status, match_details, "è®¡ç®—æ—¶é•¿ä¸­..."

        for start_time_seconds, segment in audio_segments:
            segment_duration = len(segment) / sr
            end_time_seconds = start_time_seconds + segment_duration
            if end_time_seconds > max_end_time_seconds:
                max_end_time_seconds = end_time_seconds

        print(f"[DEBUG_TIMING] éŸ³é¢‘ç‰‡æ®µæœ€å¤§ç»“æŸæ—¶é—´: {max_end_time_seconds:.2f}ç§’")
        print(f"[DEBUG_TIMING] ç†è®ºä¹è°±æ—¶é•¿: {theory_total_seconds:.2f}ç§’")

        generation_status = f"âœ… æ€»æ—¶é•¿è®¡ç®—å®Œæˆ: {max_end_time_seconds:.2f}ç§’\nğŸ§  æ­£åœ¨åˆ†é…å†…å­˜..."
        yield None, generation_status, match_details, "åˆ†é…å†…å­˜ä¸­..."

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç©ºé—´ï¼ŒåŠ ä¸Š0.5ç§’çš„ä½™é‡
        total_samples = int(max_end_time_seconds * sr) + int(0.5 * sr)
        final_audio = np.zeros(total_samples, dtype=np.float32)

        generation_status = f"âœ… å†…å­˜åˆ†é…å®Œæˆ: {total_samples}ä¸ªæ ·æœ¬\nğŸ“Œ å¼€å§‹æ”¾ç½®éŸ³é¢‘ç‰‡æ®µ..."
        yield None, generation_status, match_details, "æ”¾ç½®ç‰‡æ®µä¸­..."

        # æŒ‰æ—¶é—´çº¿æ”¾ç½®éŸ³é¢‘ç‰‡æ®µ - å…³é”®ï¼šæ‰€æœ‰æ—¶é—´éƒ½æ˜¯ç§’ï¼Œç›´æ¥ä¹˜ä»¥srå¾—åˆ°æ ·æœ¬ä½ç½®
        placed_count = 0
        for i, (start_time_seconds, segment) in enumerate(audio_segments):
            start_sample = int(start_time_seconds * sr)
            end_sample = start_sample + len(segment)

            # ç¡®ä¿ç‰‡æ®µåœ¨èŒƒå›´å†…
            if start_sample < len(final_audio):
                # è®¡ç®—å®é™…ç»“æŸä½ç½®
                end_actual = min(end_sample, len(final_audio))
                # ç¡®ä¿æ®µé•¿åº¦æ­£ç¡®
                segment_len = end_actual - start_sample
                if segment_len > 0:
                    # ä½¿ç”¨å åŠ è€Œä¸æ˜¯è¦†ç›–
                    final_audio[start_sample:end_actual] += segment[:segment_len]
                    placed_count += 1

            # æ¯æ”¾ç½®10ä¸ªç‰‡æ®µæ›´æ–°ä¸€æ¬¡çŠ¶æ€
            if i % 10 == 0 and i > 0:
                generation_status = f"ğŸ“Œ å·²æ”¾ç½® {i+1}/{len(audio_segments)} ä¸ªç‰‡æ®µ..."
                yield None, generation_status, match_details, "æ”¾ç½®ç‰‡æ®µä¸­..."

        generation_status = (
            f"âœ… ç‰‡æ®µæ”¾ç½®å®Œæˆ: {placed_count}/{len(audio_segments)} ä¸ªç‰‡æ®µ\nğŸ“ˆ æ­£åœ¨å½’ä¸€åŒ–..."
        )
        yield None, generation_status, match_details, "å½’ä¸€åŒ–ä¸­..."

        # å½’ä¸€åŒ–
        final_audio = normalize_audio(final_audio)

        # æ·»åŠ æ·¡å‡ºæ•ˆæœï¼Œé¿å…çªç„¶ç»“æŸ
        fade_out_samples = int(0.05 * sr)
        if fade_out_samples > 0 and fade_out_samples <= len(final_audio):
            fade_out_window = np.linspace(1, 0, fade_out_samples)
            final_audio[-fade_out_samples:] *= fade_out_window

        generation_status = "âœ… éŸ³é¢‘å¤„ç†å®Œæˆ\nğŸ“ æ­£åœ¨ç”ŸæˆæŠ¥å‘Š..."
        yield None, generation_status, match_details, "ç”ŸæˆæŠ¥å‘Šä¸­..."

        # 5. ç”ŸæˆæŠ¥å‘Š
        actual_duration = len(final_audio) / sr
        report = f"""
        ## ğŸ“ éŸ³ä¹ç”ŸæˆæŠ¥å‘Š

        ### åŸºæœ¬ä¿¡æ¯
        - **ä¹è°±æ–‡ä»¶**: {os.path.basename(score_file)}
        - **éŸ³ç¬¦æ€»æ•°**: {len(notes)} (å«ä¼‘æ­¢ç¬¦)
        - **å¯åŒ¹é…éŸ³ç¬¦**: {total_valid_notes} (ä¸å«ä¼‘æ­¢ç¬¦)
        - **æ¼”å¥é€Ÿåº¦**: {tempo} BPM
        - **æ¥æºåºåˆ—**: {" -> ".join(source_sequence) if source_sequence else "æœªå¯ç”¨"}
        - **ç†è®ºæ—¶é•¿**: {theory_total_seconds:.2f} ç§’
        - **å®é™…ç”Ÿæˆæ—¶é•¿**: {actual_duration:.2f} ç§’
        - **æ—¶é•¿æ¯”ä¾‹**: {actual_duration/theory_total_seconds*100:.1f}%
        - **é‡‡æ ·ç‡**: {sr} Hz

        ### åŒ¹é…æƒ…å†µ
        - **æˆåŠŸåŒ¹é…**: {matched_count} ä¸ªå¯åŒ¹é…éŸ³ç¬¦ ({match_rate:.1f}%)
        - **éœ€è¦å˜è°ƒ**: {sum(1 for n in valid_notes if n['matched'] and abs(n.get('pitch_shift', 0)) > 0.1)} ä¸ª
        - **æœªåŒ¹é…**: {total_valid_notes - matched_count} ä¸ª
        - **ä¼‘æ­¢ç¬¦**: {len(notes) - total_valid_notes} ä¸ª

        ### éŸ³é¢‘å¤„ç†
        - **ç”Ÿæˆçš„ç‰‡æ®µ**: {len(audio_segments)} ä¸ª
        - **æˆåŠŸæ”¾ç½®**: {placed_count} ä¸ªç‰‡æ®µ
        - **å³°å€¼ç”µå¹³**: {np.max(np.abs(final_audio)):.3f}

        ### ä½¿ç”¨ç‰‡æ®µ
        """

        # ç»Ÿè®¡ä½¿ç”¨çš„ç‰‡æ®µ
        used_clips = {}
        for note in valid_notes:
            if note["matched"]:
                clip_id = note["clip_id"]
                used_clips[clip_id] = used_clips.get(clip_id, 0) + 1

        for clip_id, count in used_clips.items():
            clip = clip_manager.clips[clip_id]
            note_name = clip.get("note_info", {}).get("note", "æœªçŸ¥")
            report += f"- **ç‰‡æ®µ{clip_id}** ({note_name}): ä½¿ç”¨ {count} æ¬¡\n"

        # >>> ä¿®æ”¹ç‚¹3ï¼šæ·»åŠ éŸ³è½¨ä¸ä¹å™¨ç»Ÿè®¡ <<<
        report += "\n### éŸ³è½¨ä¸ä¹å™¨ä¿¡æ¯\n"
        # ç»Ÿè®¡éŸ³è½¨
        tracks_used = set(
            n.get("track", 0) for n in notes if n.get("track") is not None
        )
        report += f"- **ä½¿ç”¨éŸ³è½¨æ•°**: {len(tracks_used)} ä¸ª\n"

        # æŒ‰éŸ³è½¨ç»Ÿè®¡éŸ³ç¬¦
        if len(tracks_used) > 1:
            report += "- **å„éŸ³è½¨éŸ³ç¬¦åˆ†å¸ƒ**:\n"
            for track_num in sorted(tracks_used):
                track_notes = [
                    n
                    for n in notes
                    if n.get("track", 0) == track_num and n["midi_pitch"] != -1
                ]
                if track_notes:
                    instr = track_notes[0].get("instrument", "unknown")
                    report += f"  - éŸ³è½¨{track_num} ({instr}): {len(track_notes)} ä¸ªéŸ³ç¬¦\n"

        # ç»Ÿè®¡ä¹å™¨ï¼ˆä»…ç»Ÿè®¡éä¼‘æ­¢ç¬¦ï¼‰
        instruments_used = {}
        for note in valid_notes:
            instr = note.get("instrument", "unknown")
            instruments_used[instr] = instruments_used.get(instr, 0) + 1

        if instruments_used:
            report += "- **ä¹å™¨åˆ†å¸ƒ**:\n"
            for instr, count in sorted(
                instruments_used.items(), key=lambda x: x[1], reverse=True
            ):
                report += f"  - {instr}: {count} ä¸ªéŸ³ç¬¦\n"

        report += "\n### è°ƒè¯•ä¿¡æ¯\n"
        report += f"- **æœ€å¤§ç»“æŸæ—¶é—´**: {max_end_time_seconds:.2f} ç§’\n"
        report += f"- **æ€»æ ·æœ¬æ•°**: {total_samples} ä¸ª\n"
        report += f"- **å®é™…éŸ³é¢‘æ—¶é•¿**: {actual_duration:.2f} ç§’\n"

        # æ˜¾ç¤ºåŸå§‹é€Ÿåº¦ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if "original_tempo" in locals():
            report += f"- **åŸå§‹ä¹è°±é€Ÿåº¦**: {original_tempo:.0f} BPM\n"
        report += f"- **å®é™…ä½¿ç”¨é€Ÿåº¦**: {tempo} BPM\n"

        report += f"\nâ±ï¸ **ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}"

        # ä¿å­˜ç”Ÿæˆçš„éŸ³ä¹
        output_filename = f"auto_composition_{time.strftime('%Y%m%d_%H%M%S')}.wav"
        output_path = os.path.join(config.output_dir, output_filename)
        sf.write(output_path, final_audio, sr)

        generation_status = f"âœ… éŸ³ä¹ç”Ÿæˆå®Œæˆï¼\nğŸ’¾ å·²ä¿å­˜è‡³: {output_filename}"

        yield (sr, final_audio), report, match_details, generation_status

    except Exception as exc:
        error_msg = f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(exc)}"
        print(f"ç”ŸæˆéŸ³ä¹å¤±è´¥: {exc}")
        import traceback

        traceback.print_exc()
        yield None, error_msg, [], "ç”Ÿæˆå¤±è´¥"
